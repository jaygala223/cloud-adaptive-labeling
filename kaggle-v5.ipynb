{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune MaskFormer for semantic segmentation\n\nIn this notebook, we'll show how to fine-tune the model on a semantic segmentation dataset. In semantic segmentation, the goal for the model is to segment general semantic categories in an image, like \"building\", \"people\", \"sky\". No distinction is made between individual instances of a certain category, i.e. we just come up with one mask for the \"people\" category for instance, not for the individual persons.\n\nMake sure to run this notebook on a GPU.\n\n## Set-up environment\n\nFirst, we install the necessary libraries. ðŸ¤—, what else? Oh yes we'll also use [Albumentations](https://albumentations.ai/), for some data augmentation to make the model more robust. You can of course use any data augmentation library of your choice.","metadata":{"id":"c19K9npzjkC2"}},{"cell_type":"code","source":"import os\ngpu=input(\"Which gpu number you would like to allocate:\")\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets albumentations","metadata":{"id":"8zjWpYFsJhYz","outputId":"acb3fb29-ca23-4959-ba77-192fba14b43d","execution":{"iopub.status.busy":"2023-08-09T09:12:41.033316Z","iopub.execute_input":"2023-08-09T09:12:41.033688Z","iopub.status.idle":"2023-08-09T09:12:55.120616Z","shell.execute_reply.started":"2023-08-09T09:12:41.033657Z","shell.execute_reply":"2023-08-09T09:12:55.119456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle timm","metadata":{"id":"yC4EAHxoTCUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load custom data","metadata":{"id":"NNJ_q19uS5nT"}},{"cell_type":"code","source":"from google.colab import files\n\nfiles.upload()\n\n! mkdir ~/.kaggle\n! cp kaggle.json ~/.kaggle/\n! chmod 600 ~/.kaggle/kaggle.json\n!kaggle datasets download -d sorour/38cloud-cloud-segmentation-in-satellite-images\n!unzip -q /content/38cloud-cloud-segmentation-in-satellite-images.zip -d /content/38-cloud-dataset","metadata":{"id":"DOxMJ8cBS72u","outputId":"727bb176-e656-49bb-e28c-98fc74d2ba3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom PIL import Image\nimport torch\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\n\nclass CloudDataset(Dataset):\n    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, pytorch=True):\n        super().__init__()\n\n        # Loop through the files in red folder and combine, into a dictionary, the other bands\n        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in r_dir.iterdir() if not f.is_dir()]\n        self.pytorch = pytorch\n\n    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n\n        files = {'red': r_file,\n                 'green':g_dir/r_file.name.replace('red', 'green'),\n                 'blue': b_dir/r_file.name.replace('red', 'blue'),\n                 'nir': nir_dir/r_file.name.replace('red', 'nir'),\n                 'gt': gt_dir/r_file.name.replace('red', 'gt')}\n\n        return files\n\n    def __len__(self):\n\n        return len(self.files)\n\n    def open_as_array(self, idx, invert=False, include_nir=False):\n\n        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                            np.array(Image.open(self.files[idx]['green'])),\n                            np.array(Image.open(self.files[idx]['blue'])),\n                           ], axis=2)\n\n        if include_nir:\n            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n\n        if invert:\n            raw_rgb = raw_rgb.transpose((2,0,1))\n\n        # normalize\n        return (raw_rgb / np.iinfo(raw_rgb.dtype).max)\n\n\n    def open_mask(self, idx, add_dims=False):\n\n        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n\n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n\n    def __getitem__(self, idx):\n\n        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=True), dtype=torch.float32)\n        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n\n        return x, y\n\n    def open_as_pil(self, idx):\n\n        arr = 256*self.open_as_array(idx)\n\n        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n\n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n\n        return s","metadata":{"id":"fRFm7IdLTe35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = Path('/content/38-cloud-dataset/38-Cloud_training')\ndata = CloudDataset(base_path/'train_red',\n                    base_path/'train_green',\n                    base_path/'train_blue',\n                    base_path/'train_nir',\n                    base_path/'train_gt')\nlen(data)","metadata":{"id":"cHi_QVj2VjL8","outputId":"dbd2f0b4-d62b-432d-8d65-454cae5146ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = data[1000]\nx.shape, y.shape","metadata":{"id":"6383xPOLVzyt","outputId":"04549b15-ef5b-4de0-d9a0-390e24ee42c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(10,9))\nax[0].imshow(data.open_as_array(630))\nax[1].imshow(data.open_mask(630))","metadata":{"id":"afK0Xy4tV7C_","outputId":"f5427e91-c880-4c70-9604-c1e58d91f98f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.open_as_pil(8399)","metadata":{"id":"rTFgxTQglZET","outputId":"1f0d7d16-c899-4491-fcc1-90a1ed506312"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\nlabels_train = []\n\nfor img, label in data:\n  tensor_to_pil = transforms.ToPILImage()\n  image = torch.tensor(label, dtype = torch.float32)\n\n  image = tensor_to_pil(image)\n\n  # Resize the image to the desired height and width (384x384 in this case)\n  image = image.resize((384, 384), Image.NEAREST)\n  labels_train.append(image)\n\nprint(len(labels_train))","metadata":{"id":"q7V-nuqQrTtT","outputId":"78d6c25d-c56d-4aab-b5a2-ae8f1312ad06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_train[:5]","metadata":{"id":"flNychgJ9J4k","outputId":"f0d5a45f-663a-433f-b44b-5fb0c228b6ca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_train = []\n\nfor i in range(len(data)):\n  images_train.append(data.open_as_pil(i))","metadata":{"id":"nlGxOIe6qNEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_train[:5]","metadata":{"id":"TEOSvjOoqad-","outputId":"820ddf2f-12d7-4c23-9e9a-20aa2d8246e5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[0][1].shape","metadata":{"id":"2mM9iz4wH68r","outputId":"d7e26e52-37ed-4c09-b185-14a70946dfc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install huggingface_hub\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"H6KiyoIfC5Uj","outputId":"ff36c7e5-b94e-44dd-9f59-58e3839b6218"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict, Image\nimport os\n\n# your images can of course have a different extension\n# semantic segmentation maps are typically stored in the png format\n\n\n\n\n# image_paths_train = [\"/content/img2.png\"]# \"path/to/image_2.jpg/jpg\", ..., \"path/to/image_n.jpg/jpg\"]\n# label_paths_train = [\"/content/img2.png\",] # \"path/to/annotation_2.png\", ..., \"path/to/annotation_n.png\"]\n\n# label_paths_train = os.listdir(\"/content/38-cloud-dataset/train_gt_png\")\n# print(sorted(label_paths_train[:]))\n\n# same for validation\n# image_paths_validation = [...]\n# label_paths_validation = [...]\n\n# def create_dataset(image_paths, label_paths):\n#     dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n#                                 \"label\": sorted(label_paths)})\n#     dataset = dataset.cast_column(\"image\", Image())\n#     dataset = dataset.cast_column(\"label\", Image())\n\n#     return dataset\n\ndef create_dataset(images, labels):\n  dataset = Dataset.from_dict({\"image\": images,\n                                \"label\": labels})\n  return dataset\n\n# step 1: create Dataset objects\ntrain_dataset = create_dataset(images_train, labels_train)\n\nprint(train_dataset)\n\n# validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n\n# step 2: create DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    # \"validation\": validation_dataset,\n  }\n)\n\nprint(dataset, dataset['train'][0])\n\n# step 3: push to hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)\ndataset.push_to_hub(\"38-cloud-train-only-v2\")","metadata":{"id":"IcXc4VHmWmRF","outputId":"6f584a50-5828-47b3-f179-6459106756b2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data\n\nNow let's the dataset from the hub.\n\n\"But how can I use my own dataset?\" Glad you asked. I wrote a detailed guide for that [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation#note-on-custom-data).","metadata":{"id":"uVT6BIdYrJFN"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"jaygala223/38-cloud-train-only-v2\")","metadata":{"id":"67bIh90zKxhG","execution":{"iopub.status.busy":"2023-08-10T07:05:30.511884Z","iopub.execute_input":"2023-08-10T07:05:30.513027Z","iopub.status.idle":"2023-08-10T07:05:58.480282Z","shell.execute_reply.started":"2023-08-10T07:05:30.512982Z","shell.execute_reply":"2023-08-10T07:05:58.479208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset['train'][0])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:05:58.481870Z","iopub.execute_input":"2023-08-10T07:05:58.482299Z","iopub.status.idle":"2023-08-10T07:05:58.511669Z","shell.execute_reply.started":"2023-08-10T07:05:58.482258Z","shell.execute_reply":"2023-08-10T07:05:58.510766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n# # simple example\n# id2label = {0: 'non-cloud', 1: 'cloud'}\n# with open('id2label.json', 'w') as fp:\n#     json.dump(id2label, fp)","metadata":{"id":"8AarvO7mLEOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"segments/sidewalk-semantic\")","metadata":{"id":"4Avn2doyzAgK","outputId":"4a897b0d-ee4a-46dc-fae7-13c1ba5af1eb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this dataset in more detail. It consists of 1000 examples:","metadata":{"id":"ZvJCtWgHQcbx"}},{"cell_type":"code","source":"dataset","metadata":{"id":"PE8wyCup0I0u","outputId":"7c9baef4-3caa-49cb-f655-2a56154cfcb4","execution":{"iopub.status.busy":"2023-08-10T07:06:02.759847Z","iopub.execute_input":"2023-08-10T07:06:02.760820Z","iopub.status.idle":"2023-08-10T07:06:02.774811Z","shell.execute_reply.started":"2023-08-10T07:06:02.760783Z","shell.execute_reply":"2023-08-10T07:06:02.770089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\n# # since some labels have no clouds ... i.e. no 1s in the map. that causes problems with the processor\n# exclude_ids = []\n\n# for idx in range(dataset['train'].num_rows):\n#     label = dataset['train'][idx]['label']\n#     label = np.array(label)\n#     uniques = np.unique(label)\n#     if sum(uniques) == 0:\n#         exclude_ids.append(idx)\n\n# # print(exclude_ids, len(exclude_ids))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:01:55.298101Z","iopub.execute_input":"2023-08-10T04:01:55.298520Z","iopub.status.idle":"2023-08-10T04:02:52.821753Z","shell.execute_reply.started":"2023-08-10T04:01:55.298488Z","shell.execute_reply":"2023-08-10T04:02:52.820610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(exclude_ids))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:03:09.208658Z","iopub.execute_input":"2023-08-10T04:03:09.209101Z","iopub.status.idle":"2023-08-10T04:03:09.215121Z","shell.execute_reply.started":"2023-08-10T04:03:09.209064Z","shell.execute_reply":"2023-08-10T04:03:09.214114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # to exclude images with no clouds (not ideal...)\n\n# dataset['train'] = dataset['train'].select(\n#     (\n#         i for i in range(dataset['train'].num_rows) \n#         if i not in set(exclude_ids)\n#     )\n# )","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:55:39.409516Z","iopub.execute_input":"2023-08-09T13:55:39.410188Z","iopub.status.idle":"2023-08-09T13:55:40.308472Z","shell.execute_reply.started":"2023-08-09T13:55:39.410153Z","shell.execute_reply":"2023-08-09T13:55:40.307508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:06:06.505839Z","iopub.execute_input":"2023-08-10T07:06:06.507338Z","iopub.status.idle":"2023-08-10T07:06:06.522506Z","shell.execute_reply.started":"2023-08-10T07:06:06.507299Z","shell.execute_reply":"2023-08-10T07:06:06.521402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset","metadata":{"execution":{"iopub.execute_input":"2023-08-08T05:39:11.260922Z","iopub.status.busy":"2023-08-08T05:39:11.260070Z","iopub.status.idle":"2023-08-08T05:39:11.280541Z","shell.execute_reply":"2023-08-08T05:39:11.279323Z","shell.execute_reply.started":"2023-08-08T05:39:11.260875Z"},"id":"-cHHLzvWLswv","outputId":"8f3b8c59-b08a-4353-cf55-79ee166e165a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle + split dataset\ndataset = dataset.shuffle(seed=1)\ndataset = dataset[\"train\"].train_test_split(test_size=0.2)\ntrain_ds = dataset[\"train\"]\ntest_ds = dataset[\"test\"]","metadata":{"id":"6bLeRHN80ziQ","execution":{"iopub.status.busy":"2023-08-10T07:06:17.236422Z","iopub.execute_input":"2023-08-10T07:06:17.236795Z","iopub.status.idle":"2023-08-10T07:06:17.295557Z","shell.execute_reply.started":"2023-08-10T07:06:17.236763Z","shell.execute_reply":"2023-08-10T07:06:17.294679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"id":"Fu0s0bbM8ShZ","outputId":"7f957fb1-2352-4132-a5e5-0fc28b9603de","execution":{"iopub.status.busy":"2023-08-10T07:06:20.602447Z","iopub.execute_input":"2023-08-10T07:06:20.602822Z","iopub.status.idle":"2023-08-10T07:06:20.609686Z","shell.execute_reply.started":"2023-08-10T07:06:20.602791Z","shell.execute_reply":"2023-08-10T07:06:20.608641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds","metadata":{"id":"eobVRENF8TmW","outputId":"a4d97afd-e6ea-461e-ae34-abed637ef30b","execution":{"iopub.status.busy":"2023-08-10T07:06:24.510125Z","iopub.execute_input":"2023-08-10T07:06:24.510493Z","iopub.status.idle":"2023-08-10T07:06:24.517959Z","shell.execute_reply.started":"2023-08-10T07:06:24.510462Z","shell.execute_reply":"2023-08-10T07:06:24.517068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n# for example in train_ds:\n#     pil_image = example['label']  # Assuming 'label' contains the PIL image\n\n#     # Convert the PIL image to a NumPy array\n#     np_array = np.array(pil_image)\n\n#     # Normalize the values in the NumPy array to be between 0 and 1\n#     normalized_array = np_array / 255.0\n\n#     # Convert the normalized NumPy array back to a PIL image\n#     example['label'] = Image.fromarray((normalized_array).astype(np.uint8))\n\n# for example in test_ds:\n#   np_label = np.array(example['label'])\n#   np_label =","metadata":{"id":"LKvaACDGXWY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at one example (images are pretty high resolution)\nexample = train_ds[1000]\nimage = example['image']\nimage","metadata":{"id":"qK1xLJh9j3Rz","outputId":"d31a695b-bad8-44b9-d2a3-ab29af8b7b59","execution":{"iopub.status.busy":"2023-08-10T07:06:30.718100Z","iopub.execute_input":"2023-08-10T07:06:30.719210Z","iopub.status.idle":"2023-08-10T07:06:30.786837Z","shell.execute_reply.started":"2023-08-10T07:06:30.719172Z","shell.execute_reply":"2023-08-10T07:06:30.785984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnp.array(image).shape","metadata":{"id":"8VU4q0kLeaLs","outputId":"8b005e74-b071-4064-e9fe-e81aae1322ce","execution":{"iopub.status.busy":"2023-08-10T07:06:34.629905Z","iopub.execute_input":"2023-08-10T07:06:34.630304Z","iopub.status.idle":"2023-08-10T07:06:34.638251Z","shell.execute_reply.started":"2023-08-10T07:06:34.630270Z","shell.execute_reply":"2023-08-10T07:06:34.637190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# load corresponding ground truth segmentation map, which includes a label per pixel\nsegmentation_map = np.array(example['label'])/255\nsegmentation_map = np.array(segmentation_map, dtype=np.uint8)\nsegmentation_map","metadata":{"id":"C5QAqS9hj6Nt","outputId":"307007da-42d6-41c9-ebdd-3430107d4a9e","execution":{"iopub.status.busy":"2023-08-10T07:06:36.082018Z","iopub.execute_input":"2023-08-10T07:06:36.085145Z","iopub.status.idle":"2023-08-10T07:06:36.100120Z","shell.execute_reply.started":"2023-08-10T07:06:36.085104Z","shell.execute_reply":"2023-08-10T07:06:36.098922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the semantic categories in this particular example.","metadata":{"id":"z0Qzp8oNapWu"}},{"cell_type":"code","source":"np.unique(segmentation_map)","metadata":{"id":"fkwjwAxtN1cv","outputId":"3405d82a-d38e-482a-c9e7-6203eb354ed8","execution":{"iopub.status.busy":"2023-08-10T07:06:37.655926Z","iopub.execute_input":"2023-08-10T07:06:37.656318Z","iopub.status.idle":"2023-08-10T07:06:37.667475Z","shell.execute_reply.started":"2023-08-10T07:06:37.656287Z","shell.execute_reply":"2023-08-10T07:06:37.666411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.unique(segmentation_map)","metadata":{"id":"BTuloL6PL4br","outputId":"18e82764-d8df-40f9-9319-2e66c1071716"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cool, but we want to know the actual class names. For that we need the id2label mapping, which is hosted in a repo on the hub.","metadata":{"id":"DSFQ-bm7a7fA"}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport json\n\nrepo_id = f\"jaygala223/38-cloud-train-only-v2\"\nfilename = \"id2label.json\"\nid2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\nid2label = {int(k):v for k,v in id2label.items()}\nprint(id2label)","metadata":{"id":"7QINpZyhL9Eo","outputId":"6472ccdb-405f-4d9b-9f15-e7356a53ee75","execution":{"iopub.status.busy":"2023-08-10T07:06:40.347464Z","iopub.execute_input":"2023-08-10T07:06:40.347832Z","iopub.status.idle":"2023-08-10T07:06:40.908393Z","shell.execute_reply.started":"2023-08-10T07:06:40.347801Z","shell.execute_reply":"2023-08-10T07:06:40.907443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import hf_hub_download\n# import json\n\n# repo_id = f\"segments/sidewalk-semantic\"\n# filename = \"id2label.json\"\n# id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n# id2label = {int(k):v for k,v in id2label.items()}\n# print(id2label)","metadata":{"id":"s5aX9nxP0kyL","outputId":"db2509f4-0784-49e8-8393-ad10ede8bdd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [id2label[label] for label in np.unique(segmentation_map)]\nprint(labels)","metadata":{"id":"MXs1zaVDMGIx","outputId":"e3197f8e-1b29-4f1b-be35-69b5b0eb76fc","execution":{"iopub.status.busy":"2023-08-10T07:06:45.427105Z","iopub.execute_input":"2023-08-10T07:06:45.427488Z","iopub.status.idle":"2023-08-10T07:06:45.434967Z","shell.execute_reply.started":"2023-08-10T07:06:45.427455Z","shell.execute_reply":"2023-08-10T07:06:45.433921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels = [id2label[label] for label in np.unique(segmentation_map)]\n# print(labels)","metadata":{"id":"-UrDj4Zj17ne","outputId":"a4578887-4151-4b68-e764-02e482f5379e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize it:","metadata":{"id":"okw_I72ua4BC"}},{"cell_type":"code","source":"def color_palette():\n    \"\"\"Color palette that maps each class to RGB values.\n\n    This one is actually taken from ADE20k.\n    \"\"\"\n    # return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n    #         [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n    #         [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n    #         [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n    #         [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n    #         [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n    #         [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n    #         [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n    #         [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n    #         [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n    #         [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n    #         [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n    #         [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n    #         [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n    #         [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n    #         [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n    #         [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n    #         [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n    #         [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n    #         [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n    #         [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n    #         [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n    #         [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n    #         [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n    #         [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n    #         [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n    #         [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n    #         [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n    #         [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n    #         [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n    #         [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n    #         [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n    #         [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n    #         [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n    #         [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n    #         [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n    #         [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n    #         [102, 255, 0], [92, 0, 255]]\n\n    #since we only have 2 classes\n    return [[102, 255, 0], [92, 0, 255]]\n\npalette = color_palette()","metadata":{"id":"g7rq8UWhlS76","execution":{"iopub.status.busy":"2023-08-10T07:06:48.623017Z","iopub.execute_input":"2023-08-10T07:06:48.625629Z","iopub.status.idle":"2023-08-10T07:06:48.633355Z","shell.execute_reply.started":"2023-08-10T07:06:48.625592Z","shell.execute_reply":"2023-08-10T07:06:48.631912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = np.array(image) * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"x88HQCNxM8t-","outputId":"455bd3f8-e651-4a9c-963b-a43e5e87ef6b","execution":{"iopub.status.busy":"2023-08-10T07:06:50.385830Z","iopub.execute_input":"2023-08-10T07:06:50.386220Z","iopub.status.idle":"2023-08-10T07:06:50.911481Z","shell.execute_reply.started":"2023-08-10T07:06:50.386185Z","shell.execute_reply":"2023-08-10T07:06:50.910616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n# for label, color in enumerate(palette):\n#     color_segmentation_map[segmentation_map - 1 == label, :] = color\n# # Convert to BGR\n# ground_truth_color_seg = color_segmentation_map[..., ::-1]\n\n# img = np.array(image) * 0.5 + ground_truth_color_seg * 0.5\n# img = img.astype(np.uint8)\n\n# plt.figure(figsize=(15, 10))\n# plt.imshow(img)\n# plt.show()","metadata":{"id":"tzDTsgIo1XIt","outputId":"2cf59b72-3e5c-4c6c-b1d2-f50bc5d200a3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create PyTorch Dataset\n\nNext, we create a standard PyTorch dataset. Each item of the dataset consists of the image and corresponding ground truth segmentation map. We also include the original image + map (before preprocessing) in order to compute metrics like mIoU.","metadata":{"id":"d_XBkIDLl-hQ"}},{"cell_type":"code","source":"# import numpy as np\n# from torch.utils.data import Dataset\n\n# class ImageSegmentationDataset(Dataset):\n#     \"\"\"Image segmentation dataset.\"\"\"\n\n#     def __init__(self, dataset, transform = None):\n#         \"\"\"\n#         Args:\n#             dataset\n#         \"\"\"\n#         self.dataset = dataset\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.dataset)\n\n#     def __getitem__(self, idx):\n#         original_image = np.array(self.dataset[idx]['image'])\n#         original_segmentation_map = np.array(self.dataset[idx]['label'])\n\n#         if self.transform is not None:\n#           transformed = self.transform(image=original_image, mask=original_segmentation_map)\n#           image, segmentation_map = transformed['image'], transformed['mask']\n#           # convert to C, H, W\n#           image = image.transpose(2,0,1)\n#           return image, segmentation_map, original_image, original_segmentation_map\n\n#         else:\n#           original_image = original_image.transpose(2,0,1)\n#           return original_image, original_segmentation_map","metadata":{"execution":{"iopub.execute_input":"2023-08-08T05:39:43.137944Z","iopub.status.busy":"2023-08-08T05:39:43.137496Z","iopub.status.idle":"2023-08-08T05:39:43.162318Z","shell.execute_reply":"2023-08-08T05:39:43.160255Z","shell.execute_reply.started":"2023-08-08T05:39:43.137903Z"},"id":"i4Eikw_Z8C4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import Dataset\n\nclass ImageSegmentationDataset(Dataset):\n    \"\"\"Image segmentation dataset.\"\"\"\n\n    def __init__(self, dataset, transform):\n        \"\"\"\n        Args:\n            dataset\n        \"\"\"\n        self.dataset = dataset\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        original_image = np.array(self.dataset[idx]['image'])\n        original_segmentation_map = np.array(self.dataset[idx]['label'])\n        \n        # adding one bottom most pixel as 255 since processor/feature_extractor \n        # wont take labels without a positive (i.e. class: 1 or cloud)\n        uniques = np.unique(original_segmentation_map)\n        if sum(uniques) == 0:\n            original_segmentation_map[-1, -1] = 255\n        \n        transformed = self.transform(image=original_image, mask=original_segmentation_map)\n        image, segmentation_map = transformed['image'], transformed['mask']\n\n        # convert to C, H, W\n        image = image.transpose(2,0,1)\n\n        return image, segmentation_map, original_image, original_segmentation_map","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:07:01.308445Z","iopub.execute_input":"2023-08-10T07:07:01.309188Z","iopub.status.idle":"2023-08-10T07:07:01.317791Z","shell.execute_reply.started":"2023-08-10T07:07:01.309153Z","shell.execute_reply":"2023-08-10T07:07:01.316867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset accepts image transformations which can be applied on both the image and the map. Here we use Albumentations, to resize, randomly crop + flip and normalize them. Data augmentation is a widely used technique in computer vision to make the model more robust.","metadata":{"id":"ErruKkq5bftg"}},{"cell_type":"code","source":"# !pip install albumentations opencv-python","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\nADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\nADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n\ntrain_transform = A.Compose([\n#     A.LongestMaxSize(max_size=384),\n#     A.RandomCrop(width=100, height=100),\n#     A.HorizontalFlip(p=0.5),\n#     A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n])\n\ntest_transform = A.Compose([\n#     A.Resize(width=100, height=100),\n#     A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n\n])\n# train_dataset = ImageSegmentationDataset(train_ds)\ntrain_dataset = ImageSegmentationDataset(train_ds, transform=train_transform)\ntest_dataset = ImageSegmentationDataset(test_ds, transform=test_transform)\n# test_dataset = ImageSegmentationDataset(test_ds)","metadata":{"id":"aP3tzVTb8hgJ","execution":{"iopub.status.busy":"2023-08-10T07:07:04.037133Z","iopub.execute_input":"2023-08-10T07:07:04.037496Z","iopub.status.idle":"2023-08-10T07:07:05.059916Z","shell.execute_reply.started":"2023-08-10T07:07:04.037465Z","shell.execute_reply":"2023-08-10T07:07:05.058919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cnt = 0\n\n# for item in train_dataset:\n#     label = item[3]\n#     uniques = np.unique(label)\n#     if sum(uniques) == 0:\n#         cnt += 1\n\n# print(cnt)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:27:58.682302Z","iopub.execute_input":"2023-08-10T04:27:58.682769Z","iopub.status.idle":"2023-08-10T04:29:45.002842Z","shell.execute_reply.started":"2023-08-10T04:27:58.682730Z","shell.execute_reply":"2023-08-10T04:29:45.001531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my_array = np.array([\n#     [1,2,3],\n#     [4,5,6],\n#     [7,8,9]\n# ])\n\n# print(my_array.shape)\n\n# my_array[-1,-1] = -1\n\n# print(my_array)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:22:55.591690Z","iopub.execute_input":"2023-08-10T04:22:55.592857Z","iopub.status.idle":"2023-08-10T04:22:55.600120Z","shell.execute_reply.started":"2023-08-10T04:22:55.592818Z","shell.execute_reply":"2023-08-10T04:22:55.598921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for item in train_dataset:\n#   print(item)","metadata":{"id":"rE6a6NmpOAml","outputId":"5b3ae981-4b61-4856-c796-40fa44009c06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image, segmentation_map, _, _ = train_dataset[0]\nimage, segmentation_map, _, _ = train_dataset[222]\nprint(image.shape)\nprint(segmentation_map.shape)","metadata":{"id":"Dr99XeHUNg0m","outputId":"f90ea69a-2ea6-403e-f9fc-1d397fcd5804","execution":{"iopub.status.busy":"2023-08-10T07:07:07.080562Z","iopub.execute_input":"2023-08-10T07:07:07.083325Z","iopub.status.idle":"2023-08-10T07:07:07.118473Z","shell.execute_reply.started":"2023-08-10T07:07:07.083282Z","shell.execute_reply":"2023-08-10T07:07:07.117505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image, segmentation_map, _, _ = train_dataset[0]\n# print(image.shape)\n# print(segmentation_map.shape)","metadata":{"id":"JBE0WOQV8uXs","outputId":"7ab70a2a-0859-4d2a-d5cc-e42c1f2d9405"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A great way to check that our data augmentations are working well is by denormalizing the pixel values. So here we perform the inverse operation of Albumentations' normalize method and visualize the image:","metadata":{"id":"6tXh3ml3cFtO"}},{"cell_type":"code","source":"from PIL import Image\n\nunnormalized_image = (image * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nImage.fromarray(unnormalized_image)","metadata":{"execution":{"iopub.execute_input":"2023-08-08T05:39:51.739133Z","iopub.status.busy":"2023-08-08T05:39:51.738761Z","iopub.status.idle":"2023-08-08T05:39:51.848885Z","shell.execute_reply":"2023-08-08T05:39:51.847773Z","shell.execute_reply.started":"2023-08-08T05:39:51.739102Z"},"id":"IYzfYUemNspT","outputId":"f124b253-333d-47fa-a283-acab366141ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n\n# unnormalized_image = (image * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n# unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n# unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n# Image.fromarray(unnormalized_image)","metadata":{"id":"PXmOsNRS8xle","outputId":"ddd47be1-716f-4a1a-e2d6-2f61adc8ab7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This looks ok. Let's also verify whether the corresponding ground truth map is still ok.","metadata":{"id":"agSfvkzHcQVL"}},{"cell_type":"code","source":"segmentation_map.shape","metadata":{"id":"qnuSFGPRNysg","outputId":"ee178d2b-727d-49d6-db0b-032f78734b52","execution":{"iopub.status.busy":"2023-08-10T07:07:11.564939Z","iopub.execute_input":"2023-08-10T07:07:11.565645Z","iopub.status.idle":"2023-08-10T07:07:11.572758Z","shell.execute_reply.started":"2023-08-10T07:07:11.565612Z","shell.execute_reply":"2023-08-10T07:07:11.571190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segmentation_map.shape","metadata":{"id":"VmnsFA35HlVj","outputId":"24869589-79e3-4311-ff2f-c309f20f0d08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [id2label[label] for label in np.unique(segmentation_map/255.0)]\nprint(labels)","metadata":{"id":"ZfLM_S40N0bK","outputId":"831afeae-75a6-4741-e9d9-7e531b0426f0","execution":{"iopub.status.busy":"2023-08-10T07:07:13.352943Z","iopub.execute_input":"2023-08-10T07:07:13.353653Z","iopub.status.idle":"2023-08-10T07:07:13.365896Z","shell.execute_reply.started":"2023-08-10T07:07:13.353618Z","shell.execute_reply":"2023-08-10T07:07:13.364727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels = [id2label[label] for label in np.unique(segmentation_map)]\n# print(labels)","metadata":{"id":"95-nfotMICCq","outputId":"af45f503-1e3e-499d-9b38-20c2d1eadf82"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = np.moveaxis(image, 0, -1) * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"1MTYKEyfN392","outputId":"36404caa-76ce-4dc4-d5f3-0a1755e657ea","execution":{"iopub.status.busy":"2023-08-10T07:07:15.890738Z","iopub.execute_input":"2023-08-10T07:07:15.891138Z","iopub.status.idle":"2023-08-10T07:07:16.904931Z","shell.execute_reply.started":"2023-08-10T07:07:15.891103Z","shell.execute_reply":"2023-08-10T07:07:16.904083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n# for label, color in enumerate(palette):\n#     color_segmentation_map[segmentation_map == label, :] = color\n# # Convert to BGR\n# ground_truth_color_seg = color_segmentation_map[..., ::-1]\n\n# img = np.moveaxis(image, 0, -1) * 0.5 + ground_truth_color_seg * 0.5\n# img = img.astype(np.uint8)\n\n# plt.figure(figsize=(15, 10))\n# plt.imshow(img)\n# plt.show()","metadata":{"id":"cFv_KjVN87XK","outputId":"490540fa-8a40-463e-aca6-2236d97ca3c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok great!","metadata":{"id":"0HouWBIzcYwR"}},{"cell_type":"markdown","source":"## Create PyTorch DataLoaders\n\nNext we create PyTorch DataLoaders, which allow us to get batches of the dataset. For that we define a custom so-called \"collate function\", which PyTorch allows you to do. It's in this function that we'll use the preprocessor of MaskFormer, to turn the images + maps into the format that MaskFormer expects.\n\nIt's here that we make the paradigm shift that the MaskFormer authors introduced: the \"per-pixel\" annotations of the segmentation map will be turned into a set of binary masks and corresponding labels. It's this format on which we can train MaskFormer. MaskFormer namely casts any image segmentation task to this format.","metadata":{"id":"9wpLhlMWcaHl"}},{"cell_type":"code","source":"from transformers import MaskFormerImageProcessor,Mask2FormerImageProcessor, AutoImageProcessor\n\n# Create a preprocessor\n# preprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\",\n#                                                   do_reduce_labels=False,\n#                                                   do_resize=False, do_rescale=False, do_normalize=False)\n\nsize = {'longest_edge':384, 'shortest_edge':384}\n\n#original\npreprocessor = Mask2FormerImageProcessor(ignore_index=0, \n                                        do_reduce_labels=False, \n                                        do_resize=False, \n                                        do_rescale=False, \n                                        do_normalize=True,\n                                        max_size=384,\n                                        size=size)\n\n#my experiment\n# preprocessor = Mask2FormerImageProcessor(ignore_index=0, do_reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)","metadata":{"id":"N8-ThOC8ctsX","execution":{"iopub.status.busy":"2023-08-10T07:07:26.008800Z","iopub.execute_input":"2023-08-10T07:07:26.009497Z","iopub.status.idle":"2023-08-10T07:07:26.043643Z","shell.execute_reply.started":"2023-08-10T07:07:26.009462Z","shell.execute_reply":"2023-08-10T07:07:26.042567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor","metadata":{"execution":{"iopub.status.busy":"2023-08-09T09:17:17.061786Z","iopub.execute_input":"2023-08-09T09:17:17.062478Z","iopub.status.idle":"2023-08-09T09:17:17.069654Z","shell.execute_reply.started":"2023-08-09T09:17:17.062445Z","shell.execute_reply":"2023-08-09T09:17:17.068546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset[0]","metadata":{"id":"FYLs9ZC1wJ58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    inputs = list(zip(*batch))\n    images = inputs[0]\n    segmentation_maps = inputs[1]\n    # this function pads the inputs to the same size,\n    # and creates a pixel mask\n    # actually padding isn't required here since we are cropping\n    batch = preprocessor(\n        images,\n        segmentation_maps=segmentation_maps,\n        return_tensors=\"pt\",\n    )\n\n    batch[\"original_images\"] = inputs[2]\n    batch[\"original_segmentation_maps\"] = inputs[3]\n    \n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:07:31.919129Z","iopub.execute_input":"2023-08-10T07:07:31.920365Z","iopub.status.idle":"2023-08-10T07:07:31.927273Z","shell.execute_reply.started":"2023-08-10T07:07:31.920320Z","shell.execute_reply":"2023-08-10T07:07:31.926137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n\n# def collate_fn(batch):\n#     inputs = list(zip(*batch))\n#     images = inputs[0]\n#     segmentation_maps = inputs[1]\n#     # this function pads the inputs to the same size,\n#     # and creates a pixel mask\n#     # actually padding isn't required here since we are cropping\n#     batch = preprocessor(\n#         images,\n#         segmentation_maps=segmentation_maps,\n#         return_tensors=\"pt\",\n#     )\n\n#     batch[\"original_images\"] = inputs[0]\n#     batch[\"original_segmentation_maps\"] = inputs[1]\n#     print(segmentation_maps, \"\\n neeche mask labels\")\n#     print(batch['mask_labels'])\n#     return batch","metadata":{"execution":{"iopub.execute_input":"2023-08-08T06:02:49.062682Z","iopub.status.busy":"2023-08-08T06:02:49.062308Z","iopub.status.idle":"2023-08-08T06:02:49.070189Z","shell.execute_reply":"2023-08-08T06:02:49.069250Z","shell.execute_reply.started":"2023-08-08T06:02:49.062653Z"},"id":"tXCWTfTbz5Wu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n# batch size more than 2 causes CUDA out of memory error\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:20:03.422164Z","iopub.execute_input":"2023-08-10T10:20:03.423308Z","iopub.status.idle":"2023-08-10T10:20:03.429345Z","shell.execute_reply.started":"2023-08-10T10:20:03.423266Z","shell.execute_reply":"2023-08-10T10:20:03.428234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for item in train_dataloader:\n#     print(item.keys())\n#     break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:11:16.679236Z","iopub.status.idle":"2023-08-09T14:11:16.679933Z","shell.execute_reply.started":"2023-08-09T14:11:16.679693Z","shell.execute_reply":"2023-08-09T14:11:16.679715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verify data (!!)\n\nNext, it's ALWAYS very important to check whether the data you feed to the model actually makes sense. It's one of the main principles of [this amazing blog post](http://karpathy.github.io/2019/04/25/recipe/), if you wanna debug your neural networks.\n\nLet's check the first batch, and its content.","metadata":{"id":"FTFi7K4eda5W"}},{"cell_type":"code","source":"import torch\n\nbatch = next(iter(train_dataloader))\n\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k,v.shape)\n  else:\n    print(k,v[0].shape)","metadata":{"id":"X8VC2-2dOVbo","outputId":"1babd1ea-f994-401f-ac04-612015ec975d","execution":{"iopub.status.busy":"2023-08-10T10:20:05.750615Z","iopub.execute_input":"2023-08-10T10:20:05.751708Z","iopub.status.idle":"2023-08-10T10:20:05.876681Z","shell.execute_reply.started":"2023-08-10T10:20:05.751663Z","shell.execute_reply":"2023-08-10T10:20:05.875628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n\n# batch = next(iter(train_dataloader))\n# for k,v in batch.items():\n#   if isinstance(v, torch.Tensor):\n#     print(k,v.shape)\n#   else:\n#     print(k,v[0].shape)","metadata":{"id":"0Y1xFhkq0ABr","outputId":"96b9467d-711e-47c0-83c2-00fcfcdad17b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixel_values = batch[\"pixel_values\"][0].numpy()\npixel_values.shape","metadata":{"id":"xPdCw9egO3tm","outputId":"a9b9a4a7-54c6-4e8e-f07e-04c1c73c16ed","execution":{"iopub.status.busy":"2023-08-09T09:17:38.426997Z","iopub.execute_input":"2023-08-09T09:17:38.427353Z","iopub.status.idle":"2023-08-09T09:17:38.442026Z","shell.execute_reply.started":"2023-08-09T09:17:38.427325Z","shell.execute_reply":"2023-08-09T09:17:38.441011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pixel_values = batch[\"pixel_values\"][0].numpy()\n# pixel_values.shape","metadata":{"id":"s4Z5nL65J7oC","outputId":"fc88f4f7-5c5c-4662-986f-90c40352a1a2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, let's denormalize an image and see what we got.","metadata":{"id":"IvYfrGG5dsCF"}},{"cell_type":"code","source":"unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nImage.fromarray(unnormalized_image)","metadata":{"id":"RBYyIpUcO58-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n# unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n# unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n# Image.fromarray(unnormalized_image)","metadata":{"id":"oYXbVJtkJSL8","outputId":"e4bb5f73-3257-4e96-b610-ef60d16ac65f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's verify the corresponding binary masks + class labels.","metadata":{"id":"6FAF5elmd3Xc"}},{"cell_type":"code","source":"# verify class labels\nlabels = [id2label[label] for label in (batch[\"class_labels\"][0]/255.0).tolist()]\nprint(labels)","metadata":{"execution":{"iopub.execute_input":"2023-08-08T05:40:23.783603Z","iopub.status.busy":"2023-08-08T05:40:23.783192Z","iopub.status.idle":"2023-08-08T05:40:24.232604Z","shell.execute_reply":"2023-08-08T05:40:24.231192Z","shell.execute_reply.started":"2023-08-08T05:40:23.783571Z"},"id":"vdom-jV8O-KH","outputId":"4572828b-c166-4453-fe95-2fb4a5df3350"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # verify class labels\n# labels = [id2label[label] for label in batch[\"class_labels\"][0].tolist()]\n# print(labels)","metadata":{"id":"xns61BlyJjTq","outputId":"34d8f469-fd99-456c-9a17-0b9e9a24fbe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify mask labels\nbatch[\"mask_labels\"][0].shape","metadata":{"id":"5HdJFbd_O_5S","outputId":"06a1c4ac-54ef-4ab5-de83-ef37a76cae25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # verify mask labels\n# batch[\"mask_labels\"][0].shape","metadata":{"id":"aOOvaUgpJsu-","outputId":"e302b889-0af0-485a-86d6-5a5cb759ac37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_mask(labels, label_name):\n  print(\"Label:\", label_name)\n  idx = labels.index(label_name)\n\n  visual_mask = (batch[\"mask_labels\"][0][idx].bool().numpy() * 255).astype(np.uint8)\n  return Image.fromarray(visual_mask)","metadata":{"id":"PeYuEFhDKkmI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_mask(labels, \"cloud\")","metadata":{"id":"myAAfyKZPFP1","outputId":"d526ae0c-a250-47d9-8d9a-e4dd54d92d75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize_mask(labels, \"flat-road\")","metadata":{"id":"zZrDP-ozK8ze","outputId":"a8b2cd29-b962-40fd-ef8d-06725059915e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define model\n\nNext, we define the model. We equip the model with pretrained weights from the ðŸ¤— hub. We will replace only the classification head. For that we provide the id2label mapping, and specify to ignore mismatches keys to replace the already fine-tuned classification head.","metadata":{"id":"wvv5EaU5qi9M"}},{"cell_type":"code","source":"from transformers import MaskFormerForInstanceSegmentation, Mask2FormerForUniversalSegmentation\n\n# Replace the head of the pre-trained model\n\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\",\n                                                            id2label=id2label,\n                                                            ignore_mismatched_sizes=True)\n\n\n# model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\",\n#                                                           id2label=id2label,\n#                                                           ignore_mismatched_sizes=True)","metadata":{"id":"IMqBGmsFkfyB","outputId":"d9c9ad37-ad3f-4a04-fbc5-f2cbdace6110","execution":{"iopub.status.busy":"2023-08-10T10:18:09.294387Z","iopub.execute_input":"2023-08-10T10:18:09.295350Z","iopub.status.idle":"2023-08-10T10:18:10.303265Z","shell.execute_reply.started":"2023-08-10T10:18:09.295312Z","shell.execute_reply":"2023-08-10T10:18:10.302266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See also the warning here: it's telling us that we are\n\n1.   List item\n2.   List item\n\nonly replacing the class_predictor, which makes sense. As it's the only parameters that we will train from scratch.","metadata":{"id":"cDf95us4eXeZ"}},{"cell_type":"markdown","source":"## Compute initial loss\n\nAnother good way to debug neural networks is to verify the initial loss, see if it makes sense.","metadata":{"id":"oQilHqCYLJx0"}},{"cell_type":"code","source":"# v = batch[\"class_labels\"]\n\n# v = [t / 255.0 for t in v]","metadata":{"id":"zpDJ8pD6gbRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# v","metadata":{"id":"AN8joIVLhGxe","outputId":"0df061fa-ab99-430a-9e58-e0016f5b80b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# v = batch[\"class_labels\"]\n\n# v = [t / 255.0 for t in v]\n\n# new_v = []\n\n# for t in v:\n#   new_t = torch.tensor(t, dtype=torch.int64)\n#   new_v.append(new_t)","metadata":{"id":"RT-20EWxiaWe","outputId":"83f98432-5bf8-492b-8033-08a60e6612e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [torch.tensor([1.], dtype=torch.uint8)]*2","metadata":{"id":"mD6OP4chYiFo","outputId":"f5b595a0-75cb-46a0-e12a-e1702a5a0c81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\n\n# load MaskFormer fine-tuned on COCO panoptic segmentation\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\", \n                                                               size={'longest_edge':384, 'shortest_edge':383}, \n                                                               ignore_index=0)","metadata":{"id":"zoFayrmCgico","outputId":"35757a0f-9a46-44ac-d430-5eb6e98cb229","execution":{"iopub.status.busy":"2023-08-10T10:18:13.655337Z","iopub.execute_input":"2023-08-10T10:18:13.655713Z","iopub.status.idle":"2023-08-10T10:18:13.762020Z","shell.execute_reply.started":"2023-08-10T10:18:13.655681Z","shell.execute_reply":"2023-08-10T10:18:13.760922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:44:32.287544Z","iopub.execute_input":"2023-08-10T04:44:32.288286Z","iopub.status.idle":"2023-08-10T04:44:32.296903Z","shell.execute_reply.started":"2023-08-10T04:44:32.288246Z","shell.execute_reply":"2023-08-10T04:44:32.295711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\"\n\nimages, labels = batch['original_images'], batch['original_segmentation_maps']\n\n# first convert to np array then to tensor... because list to tensor is a slow operation\nimages = np.array(images)\nimages = torch.tensor(images)\nlabels = np.array(labels)\nlabels = torch.tensor(labels)/255\n\nimages.to(device)\nlabels.to(device)\nmodel.to(device)\n\ninputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n# print(inputs)\n\ninputs['mask_labels'] = torch.stack(inputs['mask_labels'])\ninputs['class_labels'] = torch.stack(inputs['class_labels'])\ninputs['pixel_values'] = inputs['pixel_values'].float()\n# print(inputs)\ninputs.to(device)\n\noutputs = model(**inputs)\n\nprint(\"done!\")","metadata":{"id":"HCKuE7eXYLCv","outputId":"fe863f37-e142-4a89-bfea-66047e626e6d","execution":{"iopub.status.busy":"2023-08-10T10:20:13.244328Z","iopub.execute_input":"2023-08-10T10:20:13.244687Z","iopub.status.idle":"2023-08-10T10:20:13.810385Z","shell.execute_reply.started":"2023-08-10T10:20:13.244656Z","shell.execute_reply":"2023-08-10T10:20:13.809366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [(i//255) for i in batch[\"class_labels\"]]","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:31:30.597573Z","iopub.execute_input":"2023-08-09T13:31:30.598434Z","iopub.status.idle":"2023-08-09T13:31:30.602830Z","shell.execute_reply.started":"2023-08-09T13:31:30.598399Z","shell.execute_reply":"2023-08-09T13:31:30.601717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs = model(batch[\"pixel_values\"].float(),\n#                 class_labels=[(i//255) for i in batch[\"class_labels\"]],\n#                 mask_labels=batch[\"mask_labels\"])","metadata":{"id":"CK2IhtcOLLEA","execution":{"iopub.status.busy":"2023-08-09T13:31:26.498837Z","iopub.execute_input":"2023-08-09T13:31:26.499221Z","iopub.status.idle":"2023-08-09T13:31:26.504107Z","shell.execute_reply.started":"2023-08-09T13:31:26.499188Z","shell.execute_reply":"2023-08-09T13:31:26.503127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.loss","metadata":{"id":"357KLDNhjUj1","outputId":"2b555db0-4640-4a62-d54b-5d70b9225176","execution":{"iopub.status.busy":"2023-08-10T10:20:16.731753Z","iopub.execute_input":"2023-08-10T10:20:16.732538Z","iopub.status.idle":"2023-08-10T10:20:16.742472Z","shell.execute_reply.started":"2023-08-10T10:20:16.732500Z","shell.execute_reply":"2023-08-10T10:20:16.741074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs.loss","metadata":{"id":"sMsSWMO-LQZB","outputId":"dd8b10a4-e0ff-409f-e410-600454becb2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n\nIt's time to train the model! We'll use the mIoU metric to track progress.","metadata":{"id":"oP4Aj19v1Da_"}},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"id":"xNIijv2LQCU4","outputId":"ae0df21a-8304-41e9-f09e-e6aead9497f7","execution":{"iopub.status.busy":"2023-08-10T07:08:37.662935Z","iopub.execute_input":"2023-08-10T07:08:37.663869Z","iopub.status.idle":"2023-08-10T07:08:49.221370Z","shell.execute_reply.started":"2023-08-10T07:08:37.663824Z","shell.execute_reply":"2023-08-10T07:08:49.220023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nmean_iou = evaluate.load(\"mean_iou\")\nprecision = evaluate.load(\"precision\")\n\nclf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\", \"mean_iou\", \"recall\"])","metadata":{"id":"nuoNVUSKP8u1","execution":{"iopub.status.busy":"2023-08-10T07:16:09.584414Z","iopub.execute_input":"2023-08-10T07:16:09.585198Z","iopub.status.idle":"2023-08-10T07:16:13.831884Z","shell.execute_reply.started":"2023-08-10T07:16:09.585158Z","shell.execute_reply":"2023-08-10T07:16:13.830899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch[\"pixel_values\"].size(0)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:56:22.573967Z","iopub.execute_input":"2023-08-10T04:56:22.574677Z","iopub.status.idle":"2023-08-10T04:56:22.581097Z","shell.execute_reply.started":"2023-08-10T04:56:22.574644Z","shell.execute_reply":"2023-08-10T04:56:22.580081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"import torch\nfrom tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = \"cuda\"\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\nrunning_loss = 0.0\nnum_samples = 0\nfor epoch in range(2):\n  print(\"Epoch:\", epoch)\n  model.train()\n  for idx, batch in enumerate(tqdm(train_dataloader)):\n      # Reset the parameter gradients\n      optimizer.zero_grad()\n\n      images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n      images = np.array(images)\n      images = torch.tensor(images)\n      labels = np.array(labels)\n      labels = torch.tensor(labels)/255\n\n      images.to(device)\n      labels.to(device)\n      model.to(device)\n\n      inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#       print(inputs)\n\n      inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n      inputs['class_labels'] = torch.stack(inputs['class_labels'])\n      inputs['pixel_values'] = inputs['pixel_values'].float()\n      inputs.to(device)\n\n      outputs = model(**inputs)\n\n      # Backward propagation\n      loss = outputs.loss\n      loss.backward()\n\n      batch_size = batch[\"pixel_values\"].size(0)\n      running_loss += loss.item()\n      num_samples += batch_size\n\n      if idx % 100 == 0:\n        print(\"Loss:\", running_loss/num_samples)\n\n      # Optimization\n      optimizer.step()\n      break\n\n  model.eval()\n  for idx, batch in enumerate(tqdm(test_dataloader)):\n    if idx > 1:\n      break\n    images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n    images = np.array(images)\n    images = torch.tensor(images)\n    labels = np.array(labels)\n    labels = torch.tensor(labels)/255\n\n    images.to(device)\n    labels.to(device)\n    model.to(device)\n\n    inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#     print(inputs)\n    inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n    inputs['class_labels'] = torch.stack(inputs['class_labels'])  \n    inputs['pixel_values'] = inputs['pixel_values'].float()\n    inputs.to(device)\n\n    # Forward pass\n    with torch.no_grad():\n      # outputs = model(pixel_values=pixel_values.to(device))\n      outputs = model(**inputs)\n\n    # get original images\n    # original_images = batch[\"original_images\"]\n    target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n    # predict segmentation maps\n    predicted_segmentation_maps = feature_extractor.post_process_semantic_segmentation(outputs,\n                                                                                  target_sizes=target_sizes)\n    \n    print(outputs.keys())\n    # get ground truth segmentation maps\n    # ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n    for preds in predicted_segmentation_maps:\n        preds.int()\n        preds.cpu()\n    for label in labels:\n        label.int()\n        label.cpu()\n        \n#     print(predicted_segmentation_maps)\n#     print(labels)\n    \n    # removing all values into a list of ints because the evaluate library expects it that way.\n    labels_for_evaluation = []\n    \n    for label in labels:\n        labels_for_evaluation.append(label.view(-1))\n    \n    labels_for_evaluation = torch.cat(labels_for_evaluation, dim = 0)\n    labels_for_evaluation.int()\n    \n    pred_labels_for_evaluation = []\n    \n    for preds in predicted_segmentation_maps:\n        pred_labels_for_evaluation.append(label.view(-1))\n    \n    pred_labels_for_evaluation = torch.cat(pred_labels_for_evaluation, dim = 0)\n    pred_labels_for_evaluation.int()\n    \n    print(labels)\n    print(predicted_segmentation_maps)\n    \n    mean_iou.add_batch(references=labels, predictions=predicted_segmentation_maps)\n    precision.add_batch(references=labels_for_evaluation, predictions=pred_labels_for_evaluation)\n#     clf_metrics.add_batch(references=labels, predictions=predicted_segmentation_maps)\n\n  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n  # so if you're interested, feel free to print them as well\n  print(\"mean IoU: \", mean_iou.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])\n  print(\"precision: \", precision.compute()['precision'])\n#   print(clf_metrics.compute(num_labels = len(id2label), ignore_index = 0))","metadata":{"id":"WsTo9eRRj96f","outputId":"130e5400-659b-4d75-f7b0-23450acadc8f","execution":{"iopub.status.busy":"2023-08-10T10:15:23.575867Z","iopub.execute_input":"2023-08-10T10:15:23.576246Z","iopub.status.idle":"2023-08-10T10:17:03.753201Z","shell.execute_reply.started":"2023-08-10T10:15:23.576215Z","shell.execute_reply":"2023-08-10T10:17:03.752132Z"}}},{"cell_type":"code","source":"print(labels_for_evaluation)\nl = torch.cat(labels_for_evaluation, dim=0)\nl.int()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:52:11.005892Z","iopub.execute_input":"2023-08-10T07:52:11.006509Z","iopub.status.idle":"2023-08-10T07:52:11.017925Z","shell.execute_reply.started":"2023-08-10T07:52:11.006475Z","shell.execute_reply":"2023-08-10T07:52:11.016763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from tqdm.auto import tqdm\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\n# running_loss = 0.0\n# num_samples = 0\n# for epoch in range(3):\n#   print(\"Epoch:\", epoch)\n#   model.train()\n#   for idx, batch in enumerate(tqdm(train_dataloader)):\n#       # Reset the parameter gradients\n#       optimizer.zero_grad()\n\n#       # Forward pass\n#       outputs = model(\n#           pixel_values=batch[\"pixel_values\"].to(device),\n#           mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n#           class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n#       )\n\n#       # Backward propagation\n#       loss = outputs.loss\n#       loss.backward()\n\n#       batch_size = batch[\"pixel_values\"].size(0)\n#       running_loss += loss.item()\n#       num_samples += batch_size\n\n#       if idx % 100 == 0:\n#         print(\"Loss:\", running_loss/num_samples)\n\n#       # Optimization\n#       optimizer.step()\n\n#   model.eval()\n#   for idx, batch in enumerate(tqdm(test_dataloader)):\n#     if idx > 5:\n#       break\n\n#     pixel_values = batch[\"pixel_values\"]\n\n#     # Forward pass\n#     with torch.no_grad():\n#       outputs = model(pixel_values=pixel_values.to(device))\n\n#     # get original images\n#     original_images = batch[\"original_images\"]\n#     target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n#     # predict segmentation maps\n#     predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs,\n#                                                                                   target_sizes=target_sizes)\n\n#     # get ground truth segmentation maps\n#     ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n\n#     metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n\n#   # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n#   # so if you're interested, feel free to print them as well\n#   print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])","metadata":{"id":"IBcCOO_91EKI","outputId":"b024f86a-e010-490c-a84f-b5a2586b67be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hi\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T16:03:10.111190Z","iopub.execute_input":"2023-08-09T16:03:10.111603Z","iopub.status.idle":"2023-08-09T16:03:10.118158Z","shell.execute_reply.started":"2023-08-09T16:03:10.111571Z","shell.execute_reply":"2023-08-09T16:03:10.116232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nAfter training, we can use the model to make predictions on new data.\n\nLet's showcase this one of the examples of a test batch.","metadata":{"id":"aWO7PYmL-B5N"}},{"cell_type":"code","source":"# let's take the first test batch\nbatch = next(iter(test_dataloader))\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k,v.shape)\n  else:\n    print(k,len(v))","metadata":{"id":"MNxV_19j-Crl","outputId":"758a30b0-bcc5-44f9-8dc4-b5afe3f6112d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# forward pass\nwith torch.no_grad():\n  outputs = model(batch[\"pixel_values\"].to(device))","metadata":{"id":"PqfrPxfe-S8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_images = batch[\"original_images\"]\ntarget_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n# predict segmentation maps\npredicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)","metadata":{"id":"j5fC9KrU-aNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = batch[\"original_images\"][0]\nImage.fromarray(image)","metadata":{"id":"adqqcnpl-jy2","outputId":"a584fc50-346d-411b-f7a1-7ee6e3c6a278"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nsegmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = image * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"tXZrYMNW-yrN","outputId":"d0c2847a-8ac7-40ac-f47a-37a41280a962"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare to the ground truth:","metadata":{"id":"hf0WXFT5_Np8"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nsegmentation_map = batch[\"original_segmentation_maps\"][0]\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = image * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"DKZ_3oF__Pll","outputId":"e37f8a4c-34cb-4c65-c198-d73eb83aa8c6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didn't do a lot of training (only 2 epochs), and results don't look too bad. I'd suggest checking the paper to find all details regarding training hyperparameters (number of epochs, learning rate, etc.).","metadata":{"id":"d9u_iFnmjPVo"}},{"cell_type":"code","source":"","metadata":{"id":"M0MIQub-RjUH"},"execution_count":null,"outputs":[]}]}