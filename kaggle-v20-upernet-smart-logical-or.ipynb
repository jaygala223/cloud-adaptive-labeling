{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers albumentations huggingface_hub","metadata":{"id":"OtXvkZL_xBWA","outputId":"fd99217e-af1c-4f40-9d5d-600831fff32a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NEW ALGO #\n\n\"\"\"\nto deal with inconsistently labelled data\nwe will train our model on the original data\nthen we will use the output maps from this model\nand combine them with original maps using (logical OR)\nthen re-train and check results\n\n\"\"\"","metadata":{"id":"y2CkGHF30xOi","outputId":"ce64203e-5abf-4520-d966-31ad90104149","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# v1 is original RGB dataset\n# v2 is with some extra preprocessing that was needed for maskformer (not needed in upernet)\n# v3 is with RGB + NIR channel dataset (poor performance on this one)\n\ndataset = load_dataset(\"jaygala223/38-cloud-train-only-v1\")","metadata":{"id":"HP5UOxNGK9lA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"id":"DYQoifJBK_BO","outputId":"5cb370d2-cae7-450e-df22-605ac4f9fa58","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # exclude bad labels\n\n# bad_labels = [35, 36]\n\n# dataset['train'] = dataset['train'].select(\n#     (\n#         i for i in range(dataset['train'].num_rows)\n#         if i not in set(bad_labels)\n#     )\n# )","metadata":{"id":"uh_9uHAV78IT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train'][2500]['image']","metadata":{"id":"8LZBNrkrjDlu","outputId":"b245d9e0-6f21-45ae-b29b-02020b61101d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['train'][2500]['label']","metadata":{"id":"f-Ioqg5FizPO","outputId":"68d33e3e-ea65-4a66-f5f1-3fe40c17b5f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle + split dataset\ndataset = dataset.shuffle(seed=1)\ndataset = dataset[\"train\"].train_test_split(test_size=0.01)\ntrain_ds = dataset[\"train\"]\ntest_ds = dataset[\"test\"]","metadata":{"id":"xPt6sgx-LCvK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_ds)\nprint(test_ds)","metadata":{"id":"KMpU8TB3LEsz","outputId":"6516b11d-6068-48e8-83f7-75e373340398","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at one example\nexample = train_ds[39]\nimage = example['image']\nimage","metadata":{"id":"x8grhQGlwQbd","outputId":"b51b8a41-9a90-4119-c29b-7a0ea4b58265","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnp.array(image).shape","metadata":{"id":"4IqBZ5xDwQ74","outputId":"030be5be-864c-4f11-da45-6a78c9f4893d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# load corresponding ground truth segmentation map, which includes a label per pixel\nsegmentation_map = np.array(example['label'])/255\nsegmentation_map = np.array(segmentation_map, dtype=np.uint8)\nsegmentation_map","metadata":{"id":"uwinrvnRwS3k","outputId":"c3086eab-ebcb-4c68-d8a1-267017436e9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(segmentation_map)","metadata":{"id":"M26RufPgwUgL","outputId":"f5352a13-f705-4859-97ec-471fefb95ae4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport json\n\nrepo_id = f\"jaygala223/38-cloud-train-only-v2\"\nfilename = \"id2label.json\"\nid2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\nid2label = {int(k):v for k,v in id2label.items()}\nprint(id2label)","metadata":{"id":"TZQm7iJZwVhW","outputId":"3a3cc70c-51ed-4c4e-ef51-2d87b9c7f09f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def color_palette():\n    \"\"\"Color palette that maps each class to RGB values.\n\n    This one is actually taken from ADE20k.\n    \"\"\"\n    # return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n    #         [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n    #         [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n    #         [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n    #         [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n    #         [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n    #         [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n    #         [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n    #         [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n    #         [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n    #         [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n    #         [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n    #         [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n    #         [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n    #         [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n    #         [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n    #         [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n    #         [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n    #         [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n    #         [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n    #         [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n    #         [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n    #         [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n    #         [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n    #         [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n    #         [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n    #         [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n    #         [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n    #         [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n    #         [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n    #         [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n    #         [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n    #         [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n    #         [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n    #         [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n    #         [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n    #         [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n    #         [102, 255, 0], [92, 0, 255]]\n\n    #since we only have 2 classes\n    return [[102, 255, 0], [92, 0, 255]]\n\npalette = color_palette()","metadata":{"id":"NYrF5ceEwWqJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = np.array(image) * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"M3fFGlWAwYBt","outputId":"a234ba1f-327a-4cca-abaa-41099da83caa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import Dataset\n\nclass ImageSegmentationDataset(Dataset):\n    \"\"\"Image segmentation dataset.\"\"\"\n\n    def __init__(self, dataset, transform):\n        \"\"\"\n        Args:\n            dataset\n        \"\"\"\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        original_image = np.array(self.dataset[idx]['image'])\n        original_segmentation_map = np.array(self.dataset[idx]['label'])\n\n        # adding one bottom most pixel as 255 since processor/feature_extractor\n        # wont take labels without a positive (i.e. class: 1 or cloud)\n        uniques = np.unique(original_segmentation_map)\n        if sum(uniques) == 0:\n            original_segmentation_map[-1, -1] = 255\n\n        transformed = self.transform(image=original_image, mask=original_segmentation_map)\n        image, segmentation_map = transformed['image'], transformed['mask']\n\n        # convert to C, H, W\n        image = image.transpose(2,0,1)\n\n        return image, segmentation_map, original_image, original_segmentation_map","metadata":{"id":"gCyXLAPOwZwH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\n\nADE_MEAN = np.array([100, 100, 100]) / 255\nADE_STD = np.array([100, 100, 100]) / 255\n\ntrain_transform = A.Compose([\n    # A.LongestMaxSize(max_size=384),\n    # A.RandomCrop(width=100, height=100),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n])\n\ntest_transform = A.Compose([\n    # A.Resize(width=100, height=100),\n    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n])\n\n# train_dataset = ImageSegmentationDataset(train_ds)\ntrain_dataset = ImageSegmentationDataset(train_ds, transform=train_transform)\ntest_dataset = ImageSegmentationDataset(test_ds, transform=test_transform)\n# test_dataset = ImageSegmentationDataset(test_ds)","metadata":{"id":"7tbKHlgCwa7l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image, segmentation_map, _, _ = train_dataset[0]\nimage, segmentation_map, _, _ = train_dataset[222]\nprint(image.shape)\nprint(segmentation_map.shape)","metadata":{"id":"YYBOwHFMwcXp","outputId":"95a1223c-d697-4a56-bcc0-b73da233ac59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MaskFormerImageProcessor,Mask2FormerImageProcessor, AutoImageProcessor\n\n# Create a preprocessor\n# preprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\",\n#                                                   do_reduce_labels=False,\n#                                                   do_resize=False, do_rescale=False, do_normalize=False)\n\nsize = {'longest_edge':384, 'shortest_edge':384}\n\n#original\npreprocessor = MaskFormerImageProcessor(ignore_index=0,\n                                        do_reduce_labels=False,\n                                        do_resize=False,\n                                        do_rescale=False,\n                                        do_normalize=True,\n                                        size=size)\n\n#my experiment\n# preprocessor = Mask2FormerImageProcessor(ignore_index=0, do_reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)","metadata":{"id":"3B_pv7bsweRu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    inputs = list(zip(*batch))\n    images = inputs[0]\n    segmentation_maps = inputs[1]\n    # this function pads the inputs to the same size,\n    # and creates a pixel mask\n    # actually padding isn't required here since we are cropping\n    batch = preprocessor(\n        images,\n        segmentation_maps=segmentation_maps,\n        return_tensors=\"pt\",\n    )\n\n    batch[\"original_images\"] = inputs[2]\n    batch[\"original_segmentation_maps\"] = inputs[3]\n\n    return batch","metadata":{"id":"qjyUPbv4wgOE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n# batch size more than 2 causes CUDA out of memory error\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn, drop_last=True)","metadata":{"id":"ksZzEQOowhct","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nbatch = next(iter(train_dataloader))\n\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k,v.shape)\n  else:\n    print(k,v[0].shape)","metadata":{"id":"LsRe9bXQwjWI","outputId":"7fee5e3f-2469-4337-987a-7e2647bceb87","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MaskFormerForInstanceSegmentation, Mask2FormerForUniversalSegmentation\n\n# Replace the head of the pre-trained model\n\n# model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n#                                                             id2label=id2label,\n#                                                             ignore_mismatched_sizes=True)\n\n\nfrom transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n\n# feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b3-finetuned-ade-512-512\")\n# model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b3-finetuned-ade-512-512\")\n\n# model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-ade-semantic\",\n#                                                           id2label=id2label,\n#                                                           ignore_mismatched_sizes=True)\n\n# model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\",\n#                                                           id2label=id2label,\n#                                                           ignore_mismatched_sizes=True)\n\n# model = MaskFormerForInstanceSegmentation.from_pretrained(\"jaygala223/maskformer-finetuned-for-38-cloud-dataset\",\n#                                                           id2label=id2label,\n#                                                           ignore_mismatched_sizes=True)","metadata":{"id":"944jVS6twl1l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation, AutoImageProcessor, UperNetForSemanticSegmentation\nfrom PIL import Image\nimport requests\n\nsize = {'longest_edge':384, 'shortest_edge':383}\n\n# load MaskFormer fine-tuned on COCO panoptic segmentation\n# feature_extractor = Mask2FormerFeatureExtractor.from_pretrained(\"facebook/maskformer-swin-base-ade\",\n#                                                                size=size,\n#                                                                )\n\n# model 1 is for generating new pred segmentation maps... model 2 is to train on new pred labels\n\nimage_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-swin-tiny\")\nmodel1 = UperNetForSemanticSegmentation.from_pretrained(\"jaygala223/upernet-swin-tiny-finetuned-for-38-cloud-dataset\")\nmodel2 = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-swin-tiny\")","metadata":{"id":"bTBEVtu3wnMj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = \"cuda\"\nbatch = next(iter(train_dataloader))\n\nimages, labels = batch['original_images'], batch['original_segmentation_maps']\n\n# first convert to np array then to tensor... because list to tensor is a slow operation\nimages = np.array(images)\nimages = torch.tensor(images)/255\nlabels = np.array(labels)\nlabels = torch.tensor(labels)/255\n\nimages.to(device)\nlabels.to(device)\nmodel1.to(device)\nmodel2.to(device)\n\ninputs1 = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\ninputs1.to(device)\n\n# print(labels.shape)\n\noutputs1 = model1(**inputs1)\n\ntarget_sizes1 = [(image.shape[0], image.shape[1]) for image in images]\n\npredicted_segmentation_maps1 = image_processor.post_process_semantic_segmentation(outputs1,\n                                                                                  target_sizes=target_sizes1)\n\n# generate new labels using OR only when the map contains 0s and 1s both\nfor label, pred_map in zip(labels, predicted_segmentation_maps1):\n    if len(torch.unique(label)) == 2:\n        label = torch.logical_or(label.to(device), pred_map.to(device))\n\n# labels = torch.logical_or(labels.to(device), torch.stack(predicted_segmentation_maps1).to(device))\n\ninputs2 = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\ninputs2.to(device)\n\n# train on new labels to tackle label inconsistency\noutputs2 = model2(**inputs2)\n\nprint(\"done!\")","metadata":{"id":"DEyiVgTkwoSn","outputId":"3b8de3ec-f27c-4706-c9c0-ca4647457f84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(outputs1.loss)\nprint(outputs2.loss)","metadata":{"id":"lCr67rDEwqHy","outputId":"9fb8eaa5-dfdf-486f-e9e4-c2dea10f41b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRAINING","metadata":{"id":"IQWKPwmEwrN9"}},{"cell_type":"code","source":"# UPER NET with new LOGICAL OR technique\n\nimport torch\nfrom tqdm.auto import tqdm\nfrom torch.optim.lr_scheduler import StepLR\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = \"cuda\"\nmodel1.to(device)\nmodel2.to(device)\n\noptimizer = torch.optim.Adam(model2.parameters(), lr=5e-5)\nscheduler = StepLR(optimizer, step_size=1000, gamma=0.1)\n\nrunning_loss = 0.0\nnum_samples = 0\n\nfor epoch in range(1):\n  print(\"Epoch:\", epoch)\n  model2.train()\n  for idx, batch in enumerate(tqdm(train_dataloader)):\n      # Reset the parameter gradients\n      optimizer.zero_grad()\n\n      images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n      images = np.array(images)\n      images = torch.tensor(images)\n      labels = np.array(labels)\n      labels = torch.tensor(labels)/255\n\n      images.to(device)\n      labels.to(device)\n\n      # forward pass\n\n      inputs1 = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\n      inputs1.to(device)\n\n      outputs1 = model1(**inputs1)\n\n      target_sizes1 = [(image.shape[0], image.shape[1]) for image in images]\n\n      predicted_segmentation_maps1 = image_processor.post_process_semantic_segmentation(outputs1,\n                                                                                  target_sizes=target_sizes1)\n\n      # generate new labels using OR only when the map contains 0s and 1s both\n      for label, pred_map in zip(labels, predicted_segmentation_maps1):\n          if len(torch.unique(label)) == 2:\n              label = torch.logical_or(label.to(device), pred_map.to(device))\n\n      # labels = torch.logical_or(labels.to(device), torch.stack(predicted_segmentation_maps1).to(device))\n\n      inputs2 = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\n      inputs2.to(device)\n\n      # train on new labels to tackle label inconsistency\n      outputs2 = model2(**inputs2)\n\n      # Backward propagation\n      loss = outputs2.loss\n      loss.backward()\n\n      batch_size = batch[\"pixel_values\"].size(0)\n      running_loss += loss.item()\n      num_samples += batch_size\n\n      if idx % 50 == 0:\n        print(\"Loss:\", running_loss/num_samples)\n\n      # Optimization\n      optimizer.step()\n\n#       scheduler.step()","metadata":{"id":"uKhfV2vWES5O","outputId":"3451bbd8-cbfe-4a6f-ab19-b7917daa29c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torch.optim.lr_scheduler import StepLR\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = \"cuda\"\nmodel1.to(device)\nmodel2.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-9)\nscheduler = StepLR(optimizer, step_size=1000, gamma=0.1)\n\nrunning_loss = 0.0\nnum_samples = 0\nfor epoch in range(1):\n  print(\"Epoch:\", epoch)\n  model.train()\n  for idx, batch in enumerate(tqdm(train_dataloader)):\n      # Reset the parameter gradients\n      optimizer.zero_grad()\n\n      images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n      images = np.array(images)\n      images = torch.tensor(images)\n      labels = np.array(labels)\n      labels = torch.tensor(labels)/255\n\n      images.to(device)\n      labels.to(device)\n\n#       inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#       print(inputs)\n\n      # UPER NET TINY\n      # inputs = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\n      # UPER NET TINY\n\n#       inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n      # comment the mask labels and class labels for segformer\n#       inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n#       inputs['class_labels'] = torch.stack(inputs['class_labels'])\n      # comment the mask labels and class labels for segformer\n\n#       inputs['pixel_values'] = inputs['pixel_values'].float()\n      # inputs.to(device)\n\n      # outputs = model(**inputs)\n\n      # Backward propagation\n      loss = outputs.loss\n      loss.backward()\n\n      batch_size = batch[\"pixel_values\"].size(0)\n      running_loss += loss.item()\n      num_samples += batch_size\n\n      if idx % 50 == 0:\n        print(\"Loss:\", running_loss/num_samples)\n\n      # Optimization\n      optimizer.step()\n\n#       scheduler.step()","metadata":{"id":"qEDfxsEwwsd_","outputId":"5c62c4a2-457a-4a84-f76d-31256f06739e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"nKiBUXDaTp-s","outputId":"8e598289-9988-4859-954b-208639ea5bb0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repo_name = \"upernet-swin-tiny-finetuned-for-38-cloud-dataset\"\nmodel2.push_to_hub(repo_name)","metadata":{"id":"AHE_z1fuwusk","outputId":"c8b3d4db-ead5-47a9-ec66-e118e55c9e5a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TESTING","metadata":{"id":"3xSl0K-5wxlJ"}},{"cell_type":"code","source":"def calculate_iou(pred_mask_list, true_mask_list):\n    total_iou = 0.0\n    num_masks = len(pred_mask_list)\n\n    for pred_mask, true_mask in zip(pred_mask_list, true_mask_list):\n        pred_mask = pred_mask.cpu()\n        true_mask = true_mask.cpu()\n        intersection = torch.logical_and(pred_mask, true_mask).sum()\n        union = torch.logical_or(pred_mask, true_mask).sum()\n\n        iou = intersection.float() / union.float()\n        total_iou += iou.item()\n\n    avg_iou = total_iou / num_masks\n    return avg_iou","metadata":{"id":"rg7wPCadwwc8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef calculate_metrics(true_labels, predicted_labels, device):\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    true_negatives = 0\n\n    for true, predicted in zip(true_labels, predicted_labels):\n        true.to(device)\n        predicted.to(device)\n        true_positives += torch.sum((predicted == 1) & (true == 1)).item()\n        false_positives += torch.sum((predicted == 1) & (true == 0)).item()\n        false_negatives += torch.sum((predicted == 0) & (true == 1)).item()\n        true_negatives += torch.sum((predicted == 0) & (true == 0)).item()\n\n    precision = true_positives / (true_positives + false_positives + 1e-7)\n    recall = true_positives / (true_positives + false_negatives + 1e-7)\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives + 1e-7)\n    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n\n    return precision, recall, accuracy, f1_score\n\n# Example usage\ntrue_labels = [torch.tensor([1, 0, 1]), torch.tensor([1, 1, 0])]\npredicted_labels = [torch.tensor([1, 0, 1]), torch.tensor([1, 1, 0])]\n\nprecision, recall, accuracy, f1_score = calculate_metrics(true_labels, predicted_labels, \"cuda\")\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1_score)","metadata":{"id":"N7lGb8VewzuI","outputId":"d12d7e6f-a4e6-41ed-bf6d-7992cb49ec2a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UPERNET evaluation WITH NEW ALGO: LOGICAL OR\n\nfrom tqdm.auto import tqdm\n\nmodel2.eval()\ndevice = \"cpu\"\nall_ious = []\nprecisions, recalls, accuracies, f1_scores = [], [], [], []\nfor idx, batch in enumerate(tqdm(test_dataloader)):\n    if idx > 10:\n        break\n    images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n    images = np.array(images)\n    images = torch.tensor(images)\n    labels = np.array(labels)\n    labels = torch.tensor(labels)/255\n\n    images.to(device)\n    labels.to(device)\n    model1.to(device)\n    model2.to(device)\n\n\n    # Forward pass\n    with torch.no_grad():\n      # forward pass\n\n      inputs1 = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\n      inputs1.to(device)\n\n      outputs1 = model1(**inputs1)\n\n      target_sizes1 = [(image.shape[0], image.shape[1]) for image in images]\n\n      predicted_segmentation_maps1 = image_processor.post_process_semantic_segmentation(outputs1,\n                                                                                        target_sizes=target_sizes1)\n\n      images = images\n\n      # generate new labels using OR\n      labels = torch.logical_or(labels.to(device), torch.stack(predicted_segmentation_maps1).to(device))\n\n      inputs2 = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\n      inputs2.to(device)\n\n      # train on new labels to tackle label inconsistency\n      outputs2 = model2(**inputs2)\n\n    # get original images\n    # original_images = batch[\"original_images\"]\n    target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n    # predict segmentation maps\n    predicted_segmentation_maps = image_processor.post_process_semantic_segmentation(outputs2,\n                                                                                  target_sizes=target_sizes)\n\n#     print(outputs.keys())\n    # get ground truth segmentation maps\n    # ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n    for preds in predicted_segmentation_maps:\n        preds.int()\n        preds.to(device)\n    for label in labels:\n        label.int()\n        label.to(device)\n\n#     print(predicted_segmentation_maps)\n#     print(labels)\n\n    # removing all values into a list of ints because the evaluate library expects it that way.\n    labels_for_evaluation = []\n\n    for label in labels:\n        labels_for_evaluation.append(label.view(-1))\n\n    labels_for_evaluation = torch.cat(labels_for_evaluation, dim = 0)\n    labels_for_evaluation.int()\n\n    pred_labels_for_evaluation = []\n\n    for preds in predicted_segmentation_maps:\n        pred_labels_for_evaluation.append(label.view(-1))\n\n    pred_labels_for_evaluation = torch.cat(pred_labels_for_evaluation, dim = 0)\n    pred_labels_for_evaluation.int()\n\n    #for mean iou calculation... pred maps and labels must be same shape\n    labels_list = []\n\n    for i in range(labels.size(0)):\n        labels[i].int()\n        labels_list.append(labels[i])\n\n#     print(predicted_segmentation_maps, labels_list)\n    all_ious.append(calculate_iou(predicted_segmentation_maps, labels_list))\n    precision, recall, accuracy, f1_score = calculate_metrics(labels_list, predicted_segmentation_maps, device)\n    precisions.append(precision)\n    recalls.append(recall)\n    accuracies.append(accuracy)\n    f1_scores.append(f1_score)\n\nprint(\"precision\", sum(precisions)/ len(precisions))\nprint(\"f1_score\", sum(f1_scores)/ len(f1_scores))\nprint(\"recall\", sum(recalls)/ len(recalls))\nprint(\"accuracy\", sum(accuracies)/ len(accuracies))\nprint(\"mIoU: \", sum(all_ious)/len(all_ious))","metadata":{"id":"gg1psZc3QicD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nmodel2.eval()\ndevice = \"cpu\"\nall_ious = []\nprecisions, recalls, accuracies, f1_scores = [], [], [], []\nfor idx, batch in enumerate(tqdm(test_dataloader)):\n    if idx > 10:\n        break\n    images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n    images = np.array(images)\n    images = torch.tensor(images)\n    labels = np.array(labels)\n    labels = torch.tensor(labels)/255\n\n    images.to(device)\n    labels.to(device)\n    model2.to(device)\n\n    ### Upernet\n    inputs = image_processor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n\n    ### upernet\n\n#     inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#     print(inputs)\n\n    ##### comment these 2 lines for segformer... keep otherwise ####\n#     inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n#     inputs['class_labels'] = torch.stack(inputs['class_labels'])\n    ##### comment these 2 lines for segformer... keep otherwise ####\n#     inputs['pixel_values'] = inputs['pixel_values'].float()\n    inputs.to(device)\n\n    # Forward pass\n    with torch.no_grad():\n      # outputs = model(pixel_values=pixel_values.to(device))\n      outputs = model2(**inputs)\n\n    # get original images\n    # original_images = batch[\"original_images\"]\n    target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n    # predict segmentation maps\n    predicted_segmentation_maps = image_processor.post_process_semantic_segmentation(outputs,\n                                                                                  target_sizes=target_sizes)\n\n#     print(outputs.keys())\n    # get ground truth segmentation maps\n    # ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n    for preds in predicted_segmentation_maps:\n        preds.int()\n        preds.to(device)\n    for label in labels:\n        label.int()\n        label.to(device)\n\n#     print(predicted_segmentation_maps)\n#     print(labels)\n\n    # removing all values into a list of ints because the evaluate library expects it that way.\n    labels_for_evaluation = []\n\n    for label in labels:\n        labels_for_evaluation.append(label.view(-1))\n\n    labels_for_evaluation = torch.cat(labels_for_evaluation, dim = 0)\n    labels_for_evaluation.int()\n\n    pred_labels_for_evaluation = []\n\n    for preds in predicted_segmentation_maps:\n        pred_labels_for_evaluation.append(label.view(-1))\n\n    pred_labels_for_evaluation = torch.cat(pred_labels_for_evaluation, dim = 0)\n    pred_labels_for_evaluation.int()\n\n    #for mean iou calculation... pred maps and labels must be same shape\n    labels_list = []\n\n    for i in range(labels.size(0)):\n        labels[i].int()\n        labels_list.append(labels[i])\n\n#     print(predicted_segmentation_maps, labels_list)\n    all_ious.append(calculate_iou(predicted_segmentation_maps, labels_list))\n    precision, recall, accuracy, f1_score = calculate_metrics(labels_list, predicted_segmentation_maps, device)\n    precisions.append(precision)\n    recalls.append(recall)\n    accuracies.append(accuracy)\n    f1_scores.append(f1_score)\n\nprint(\"precision\", sum(precisions)/ len(precisions))\nprint(\"f1_score\", sum(f1_scores)/ len(f1_scores))\nprint(\"recall\", sum(recalls)/ len(recalls))\nprint(\"accuracy\", sum(accuracies)/ len(accuracies))\nprint(\"mIoU: \", sum(all_ious)/len(all_ious))","metadata":{"id":"8Qd17ar7w2Ae","outputId":"55eff49f-81c3-421a-d3e9-cc1ebe6bd537","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# upernet-swin tiny (tuned)\nprecision 0.9392357746591729\nf1_score 0.8274812357961276\nrecall 0.8020496805250736\naccuracy 0.9476910215432391\nmIoU:  0.4426152956756679\n\n# upernet-swin tiny (tuned)\nprecision 0.8683672456144808\nf1_score 0.7479758424948\nrecall 0.7284804817339592\naccuracy 0.9722921968707247\nmIoU:  0.5187820670279589\n    \n# Upernet with LOGICAL OR (OR applied in both training and testing)\nAt 20% training loss\nprecision 0.8409011146308205\nf1_score 0.8532656625558744\nrecall 0.8737207014226013\naccuracy 0.9673955127443059\nmIoU:  0.4893365881659768\n\nAt 12% training loss\nprecision 0.8699746802445684\nf1_score 0.8642844356050445\nrecall 0.8592141353492534\naccuracy 0.9738575328476813\nmIoU:  0.5058126130395315\n\nAt 8% training loss\nprecision 0.8674168711460868\nf1_score 0.8635056062496748\nrecall 0.8602445691426681\naccuracy 0.9734111747353077\nmIoU:  0.5060400659726425\n    \n    \n# Upernet with LOGICAL OR TWICE(OR applied in both training and testing)\nat 40% training loss\nprecision 0.8800218235572824\nf1_score 0.8670481177351661\nrecall 0.8570141818017505\naccuracy 0.9781614361383997\nmIoU:  0.512333486567844\n\n    \n# Upernet with LOGICAL OR (OR applied only in training and not in testing)\nAt 20% training loss\nprecision 0.8056717629325668\nf1_score 0.8335686847209157\nrecall 0.8731451883300031\naccuracy 0.9480223992855058\nmIoU:  0.467702101577412\n\nAt 12% training loss\nprecision 0.8366282128662313\nf1_score 0.8486793148867103\nrecall 0.8624495891429831\naccuracy 0.9583616931025791\nmIoU:  0.4875807930630716\n\n    \n# upernet swin large (untuned)\nprecision 0.04674390520428237\nf1_score 0.05677240963341936\nrecall 0.07227933222051418\naccuracy 0.1463044294935711\nmIoU:  0.33444785990053044\n\n#maskformer tuned\nprecision 0.7754642685034004\nf1_score 0.7528422777953474\nrecall 0.7397811585765738\naccuracy 0.9775063871126577\nmIoU:  0.7168471569364722\n\nprecision 0.5217402294425123\nf1_score 0.5894180085336521\nrecall 0.999999981816197\naccuracy 0.5217402294425123\nmIoU:  0.521740224174788","metadata":{"id":"iXyF-jnyWxUM","outputId":"384798fe-da1c-4b5e-9f58-573286b61a60","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* precision 0.3111225918183273\n* f1_score 0.3191659886291905\n* recall 0.3295510091217397\n* accuracy 0.9829524113575062\n* mIoU:  0.29805679046190703","metadata":{"id":"gZnRfZ3nTClx"}},{"cell_type":"markdown","source":"* precision 0.26088595055841357\n* f1_score 0.29013017962476445\n* recall 0.3274049580908574\n* accuracy 0.988511827256274\n* mIoU:  0.25725841522216797\n\n","metadata":{"id":"qBIIwKhBElr5"}}]}