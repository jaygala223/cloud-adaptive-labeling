{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune MaskFormer for semantic segmentation\n\nIn this notebook, we'll show how to fine-tune the model on a semantic segmentation dataset. In semantic segmentation, the goal for the model is to segment general semantic categories in an image, like \"building\", \"people\", \"sky\". No distinction is made between individual instances of a certain category, i.e. we just come up with one mask for the \"people\" category for instance, not for the individual persons.\n\nMake sure to run this notebook on a GPU.\n\n## Set-up environment\n\nFirst, we install the necessary libraries. ðŸ¤—, what else? Oh yes we'll also use [Albumentations](https://albumentations.ai/), for some data augmentation to make the model more robust. You can of course use any data augmentation library of your choice.","metadata":{"id":"c19K9npzjkC2"}},{"cell_type":"code","source":"# import os\n# gpu=input(\"Which gpu number you would like to allocate:\")\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets albumentations","metadata":{"id":"8zjWpYFsJhYz","outputId":"acb3fb29-ca23-4959-ba77-192fba14b43d","execution":{"iopub.status.busy":"2023-08-09T09:12:41.033316Z","iopub.execute_input":"2023-08-09T09:12:41.033688Z","iopub.status.idle":"2023-08-09T09:12:55.120616Z","shell.execute_reply.started":"2023-08-09T09:12:41.033657Z","shell.execute_reply":"2023-08-09T09:12:55.119456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q kaggle timm","metadata":{"id":"yC4EAHxoTCUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load custom data","metadata":{"id":"NNJ_q19uS5nT"}},{"cell_type":"code","source":"from google.colab import files\n\nfiles.upload()\n\n! mkdir ~/.kaggle\n! cp kaggle.json ~/.kaggle/\n! chmod 600 ~/.kaggle/kaggle.json\n!kaggle datasets download -d sorour/38cloud-cloud-segmentation-in-satellite-images\n!unzip -q /content/38cloud-cloud-segmentation-in-satellite-images.zip -d /content/38-cloud-dataset","metadata":{"id":"DOxMJ8cBS72u","outputId":"727bb176-e656-49bb-e28c-98fc74d2ba3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom PIL import Image\nimport torch\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\n\nclass CloudDataset(Dataset):\n    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, pytorch=True):\n        super().__init__()\n\n        # Loop through the files in red folder and combine, into a dictionary, the other bands\n        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in r_dir.iterdir() if not f.is_dir()]\n        self.pytorch = pytorch\n\n    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n\n        files = {'red': r_file,\n                 'green':g_dir/r_file.name.replace('red', 'green'),\n                 'blue': b_dir/r_file.name.replace('red', 'blue'),\n                 'nir': nir_dir/r_file.name.replace('red', 'nir'),\n                 'gt': gt_dir/r_file.name.replace('red', 'gt')}\n\n        return files\n\n    def __len__(self):\n\n        return len(self.files)\n\n    def open_as_array(self, idx, invert=False, include_nir=False):\n\n        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                            np.array(Image.open(self.files[idx]['green'])),\n                            np.array(Image.open(self.files[idx]['blue'])),\n                           ], axis=2)\n\n        if include_nir:\n            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n\n        if invert:\n            raw_rgb = raw_rgb.transpose((2,0,1))\n\n        # normalize\n        return (raw_rgb / np.iinfo(raw_rgb.dtype).max)\n\n\n    def open_mask(self, idx, add_dims=False):\n\n        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n\n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n\n    def __getitem__(self, idx):\n\n        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=True), dtype=torch.float32)\n        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n\n        return x, y\n\n    def open_as_pil(self, idx):\n\n        arr = 256*self.open_as_array(idx)\n\n        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n\n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n\n        return s","metadata":{"id":"fRFm7IdLTe35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = Path('/content/38-cloud-dataset/38-Cloud_training')\ndata = CloudDataset(base_path/'train_red',\n                    base_path/'train_green',\n                    base_path/'train_blue',\n                    base_path/'train_nir',\n                    base_path/'train_gt')\nlen(data)","metadata":{"id":"cHi_QVj2VjL8","outputId":"dbd2f0b4-d62b-432d-8d65-454cae5146ba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = data[1000]\nx.shape, y.shape","metadata":{"id":"6383xPOLVzyt","outputId":"04549b15-ef5b-4de0-d9a0-390e24ee42c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(10,9))\nax[0].imshow(data.open_as_array(630))\nax[1].imshow(data.open_mask(630))","metadata":{"id":"afK0Xy4tV7C_","outputId":"f5427e91-c880-4c70-9604-c1e58d91f98f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.open_as_pil(8399)","metadata":{"id":"rTFgxTQglZET","outputId":"1f0d7d16-c899-4491-fcc1-90a1ed506312"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\nlabels_train = []\n\nfor img, label in data:\n  tensor_to_pil = transforms.ToPILImage()\n  image = torch.tensor(label, dtype = torch.float32)\n\n  image = tensor_to_pil(image)\n\n  # Resize the image to the desired height and width (384x384 in this case)\n  image = image.resize((384, 384), Image.NEAREST)\n  labels_train.append(image)\n\nprint(len(labels_train))","metadata":{"id":"q7V-nuqQrTtT","outputId":"78d6c25d-c56d-4aab-b5a2-ae8f1312ad06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_train[:5]","metadata":{"id":"flNychgJ9J4k","outputId":"f0d5a45f-663a-433f-b44b-5fb0c228b6ca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_train = []\n\nfor i in range(len(data)):\n  images_train.append(data.open_as_pil(i))","metadata":{"id":"nlGxOIe6qNEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_train[:5]","metadata":{"id":"TEOSvjOoqad-","outputId":"820ddf2f-12d7-4c23-9e9a-20aa2d8246e5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[0][1].shape","metadata":{"id":"2mM9iz4wH68r","outputId":"d7e26e52-37ed-4c09-b185-14a70946dfc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install huggingface_hub\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"H6KiyoIfC5Uj","outputId":"ff36c7e5-b94e-44dd-9f59-58e3839b6218"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict, Image\nimport os\n\n# your images can of course have a different extension\n# semantic segmentation maps are typically stored in the png format\n\n\n\n\n# image_paths_train = [\"/content/img2.png\"]# \"path/to/image_2.jpg/jpg\", ..., \"path/to/image_n.jpg/jpg\"]\n# label_paths_train = [\"/content/img2.png\",] # \"path/to/annotation_2.png\", ..., \"path/to/annotation_n.png\"]\n\n# label_paths_train = os.listdir(\"/content/38-cloud-dataset/train_gt_png\")\n# print(sorted(label_paths_train[:]))\n\n# same for validation\n# image_paths_validation = [...]\n# label_paths_validation = [...]\n\n# def create_dataset(image_paths, label_paths):\n#     dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n#                                 \"label\": sorted(label_paths)})\n#     dataset = dataset.cast_column(\"image\", Image())\n#     dataset = dataset.cast_column(\"label\", Image())\n\n#     return dataset\n\ndef create_dataset(images, labels):\n  dataset = Dataset.from_dict({\"image\": images,\n                                \"label\": labels})\n  return dataset\n\n# step 1: create Dataset objects\ntrain_dataset = create_dataset(images_train, labels_train)\n\nprint(train_dataset)\n\n# validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n\n# step 2: create DatasetDict\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    # \"validation\": validation_dataset,\n  }\n)\n\nprint(dataset, dataset['train'][0])\n\n# step 3: push to hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)\ndataset.push_to_hub(\"38-cloud-train-only-v2\")","metadata":{"id":"IcXc4VHmWmRF","outputId":"6f584a50-5828-47b3-f179-6459106756b2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data\n\nNow let's the dataset from the hub.\n\n\"But how can I use my own dataset?\" Glad you asked. I wrote a detailed guide for that [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation#note-on-custom-data).","metadata":{"id":"uVT6BIdYrJFN"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"jaygala223/38-cloud-train-only-v2\")","metadata":{"id":"67bIh90zKxhG","execution":{"iopub.status.busy":"2023-08-12T08:31:01.703437Z","iopub.execute_input":"2023-08-12T08:31:01.704043Z","iopub.status.idle":"2023-08-12T08:31:24.953938Z","shell.execute_reply.started":"2023-08-12T08:31:01.704006Z","shell.execute_reply":"2023-08-12T08:31:24.952982Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset parquet/jaygala223--38-cloud-train-only-v2 to /root/.cache/huggingface/datasets/parquet/jaygala223--38-cloud-train-only-v2-8694e732c8608fcc/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fa0bec3acb143eb89be6e039608e8d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7023ab574744918aa310fb67c9d350b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/377M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268d03cc34c947d1a00e654c33344555"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86a7d795d884b3f955b685a7a668eb0"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/jaygala223--38-cloud-train-only-v2-8694e732c8608fcc/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3daf554a3db4e1781ff4096a8a80276"}},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['train'][0])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T10:32:48.273646Z","iopub.execute_input":"2023-08-10T10:32:48.274660Z","iopub.status.idle":"2023-08-10T10:32:48.298723Z","shell.execute_reply.started":"2023-08-10T10:32:48.274626Z","shell.execute_reply":"2023-08-10T10:32:48.297650Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=384x384 at 0x7ED38A7D9360>, 'label': <PIL.PngImagePlugin.PngImageFile image mode=L size=384x384 at 0x7ED38A7D9240>}\n","output_type":"stream"}]},{"cell_type":"code","source":"# import json\n# # simple example\n# id2label = {0: 'non-cloud', 1: 'cloud'}\n# with open('id2label.json', 'w') as fp:\n#     json.dump(id2label, fp)","metadata":{"id":"8AarvO7mLEOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# dataset = load_dataset(\"segments/sidewalk-semantic\")","metadata":{"id":"4Avn2doyzAgK","outputId":"4a897b0d-ee4a-46dc-fae7-13c1ba5af1eb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at this dataset in more detail. It consists of 1000 examples:","metadata":{"id":"ZvJCtWgHQcbx"}},{"cell_type":"code","source":"dataset","metadata":{"id":"PE8wyCup0I0u","outputId":"7c9baef4-3caa-49cb-f655-2a56154cfcb4","execution":{"iopub.status.busy":"2023-08-12T08:37:22.070450Z","iopub.execute_input":"2023-08-12T08:37:22.071296Z","iopub.status.idle":"2023-08-12T08:37:22.085514Z","shell.execute_reply.started":"2023-08-12T08:37:22.071254Z","shell.execute_reply":"2023-08-12T08:37:22.084554Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 8400\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# dataset","metadata":{"execution":{"iopub.execute_input":"2023-08-08T05:39:11.260922Z","iopub.status.busy":"2023-08-08T05:39:11.260070Z","iopub.status.idle":"2023-08-08T05:39:11.280541Z","shell.execute_reply":"2023-08-08T05:39:11.279323Z","shell.execute_reply.started":"2023-08-08T05:39:11.260875Z"},"id":"-cHHLzvWLswv","outputId":"8f3b8c59-b08a-4353-cf55-79ee166e165a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle + split dataset\ndataset = dataset.shuffle(seed=1)\ndataset = dataset[\"train\"].train_test_split(test_size=0.2)\ntrain_ds = dataset[\"train\"]\ntest_ds = dataset[\"test\"]","metadata":{"id":"6bLeRHN80ziQ","execution":{"iopub.status.busy":"2023-08-12T08:37:24.362054Z","iopub.execute_input":"2023-08-12T08:37:24.362796Z","iopub.status.idle":"2023-08-12T08:37:24.424799Z","shell.execute_reply.started":"2023-08-12T08:37:24.362759Z","shell.execute_reply":"2023-08-12T08:37:24.423701Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"id":"Fu0s0bbM8ShZ","outputId":"7f957fb1-2352-4132-a5e5-0fc28b9603de","execution":{"iopub.status.busy":"2023-08-12T08:37:26.969286Z","iopub.execute_input":"2023-08-12T08:37:26.969647Z","iopub.status.idle":"2023-08-12T08:37:26.976388Z","shell.execute_reply.started":"2023-08-12T08:37:26.969617Z","shell.execute_reply":"2023-08-12T08:37:26.975258Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'label'],\n    num_rows: 6720\n})"},"metadata":{}}]},{"cell_type":"code","source":"test_ds","metadata":{"id":"eobVRENF8TmW","outputId":"a4d97afd-e6ea-461e-ae34-abed637ef30b","execution":{"iopub.status.busy":"2023-08-12T08:37:28.933002Z","iopub.execute_input":"2023-08-12T08:37:28.933374Z","iopub.status.idle":"2023-08-12T08:37:28.940079Z","shell.execute_reply.started":"2023-08-12T08:37:28.933344Z","shell.execute_reply":"2023-08-12T08:37:28.939093Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'label'],\n    num_rows: 1680\n})"},"metadata":{}}]},{"cell_type":"code","source":"# from PIL import Image\n# for example in train_ds:\n#     pil_image = example['label']  # Assuming 'label' contains the PIL image\n\n#     # Convert the PIL image to a NumPy array\n#     np_array = np.array(pil_image)\n\n#     # Normalize the values in the NumPy array to be between 0 and 1\n#     normalized_array = np_array / 255.0\n\n#     # Convert the normalized NumPy array back to a PIL image\n#     example['label'] = Image.fromarray((normalized_array).astype(np.uint8))\n\n# for example in test_ds:\n#   np_label = np.array(example['label'])\n#   np_label =","metadata":{"id":"LKvaACDGXWY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at one example (images are pretty high resolution)\nexample = train_ds[1008]\nimage = example['image']\nimage","metadata":{"id":"qK1xLJh9j3Rz","outputId":"d31a695b-bad8-44b9-d2a3-ab29af8b7b59","execution":{"iopub.status.busy":"2023-08-12T08:37:30.676848Z","iopub.execute_input":"2023-08-12T08:37:30.677243Z","iopub.status.idle":"2023-08-12T08:37:30.700417Z","shell.execute_reply.started":"2023-08-12T08:37:30.677201Z","shell.execute_reply":"2023-08-12T08:37:30.699528Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<PIL.PngImagePlugin.PngImageFile image mode=RGB size=384x384>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAGACAIAAAArpSLoAAABxElEQVR4nO3BgQAAAADDoPlTX+AIVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHwDwdoAAfZN6/0AAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nnp.array(image).shape","metadata":{"id":"8VU4q0kLeaLs","outputId":"8b005e74-b071-4064-e9fe-e81aae1322ce","execution":{"iopub.status.busy":"2023-08-12T08:37:32.972902Z","iopub.execute_input":"2023-08-12T08:37:32.973291Z","iopub.status.idle":"2023-08-12T08:37:32.981306Z","shell.execute_reply.started":"2023-08-12T08:37:32.973260Z","shell.execute_reply":"2023-08-12T08:37:32.980230Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(384, 384, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\n# load corresponding ground truth segmentation map, which includes a label per pixel\nsegmentation_map = np.array(example['label'])/255\nsegmentation_map = np.array(segmentation_map, dtype=np.uint8)\nsegmentation_map","metadata":{"id":"C5QAqS9hj6Nt","outputId":"307007da-42d6-41c9-ebdd-3430107d4a9e","execution":{"iopub.status.busy":"2023-08-12T08:37:34.332297Z","iopub.execute_input":"2023-08-12T08:37:34.332666Z","iopub.status.idle":"2023-08-12T08:37:34.341308Z","shell.execute_reply.started":"2023-08-12T08:37:34.332635Z","shell.execute_reply":"2023-08-12T08:37:34.340254Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's look at the semantic categories in this particular example.","metadata":{"id":"z0Qzp8oNapWu"}},{"cell_type":"code","source":"np.unique(segmentation_map)","metadata":{"id":"fkwjwAxtN1cv","outputId":"3405d82a-d38e-482a-c9e7-6203eb354ed8","execution":{"iopub.status.busy":"2023-08-12T08:37:36.395974Z","iopub.execute_input":"2023-08-12T08:37:36.396414Z","iopub.status.idle":"2023-08-12T08:37:36.408873Z","shell.execute_reply.started":"2023-08-12T08:37:36.396377Z","shell.execute_reply":"2023-08-12T08:37:36.407685Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([0], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"# np.unique(segmentation_map)","metadata":{"id":"BTuloL6PL4br","outputId":"18e82764-d8df-40f9-9319-2e66c1071716"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cool, but we want to know the actual class names. For that we need the id2label mapping, which is hosted in a repo on the hub.","metadata":{"id":"DSFQ-bm7a7fA"}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport json\n\nrepo_id = f\"jaygala223/38-cloud-train-only-v2\"\nfilename = \"id2label.json\"\nid2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\nid2label = {int(k):v for k,v in id2label.items()}\nprint(id2label)","metadata":{"id":"7QINpZyhL9Eo","outputId":"6472ccdb-405f-4d9b-9f15-e7356a53ee75","execution":{"iopub.status.busy":"2023-08-12T08:37:37.442694Z","iopub.execute_input":"2023-08-12T08:37:37.443060Z","iopub.status.idle":"2023-08-12T08:37:37.747633Z","shell.execute_reply.started":"2023-08-12T08:37:37.443030Z","shell.execute_reply":"2023-08-12T08:37:37.746550Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)e/main/id2label.json:   0%|          | 0.00/32.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"587e051703454f75a8c15295ceb5ca80"}},"metadata":{}},{"name":"stdout","text":"{0: 'non-cloud', 1: 'cloud'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# from huggingface_hub import hf_hub_download\n# import json\n\n# repo_id = f\"segments/sidewalk-semantic\"\n# filename = \"id2label.json\"\n# id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n# id2label = {int(k):v for k,v in id2label.items()}\n# print(id2label)","metadata":{"id":"s5aX9nxP0kyL","outputId":"db2509f4-0784-49e8-8393-ad10ede8bdd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [id2label[label] for label in np.unique(segmentation_map)]\nprint(labels)","metadata":{"id":"MXs1zaVDMGIx","outputId":"e3197f8e-1b29-4f1b-be35-69b5b0eb76fc","execution":{"iopub.status.busy":"2023-08-12T08:37:39.548254Z","iopub.execute_input":"2023-08-12T08:37:39.548628Z","iopub.status.idle":"2023-08-12T08:37:39.558137Z","shell.execute_reply.started":"2023-08-12T08:37:39.548601Z","shell.execute_reply":"2023-08-12T08:37:39.556952Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['non-cloud']\n","output_type":"stream"}]},{"cell_type":"code","source":"# labels = [id2label[label] for label in np.unique(segmentation_map)]\n# print(labels)","metadata":{"id":"-UrDj4Zj17ne","outputId":"a4578887-4151-4b68-e764-02e482f5379e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize it:","metadata":{"id":"okw_I72ua4BC"}},{"cell_type":"code","source":"def color_palette():\n    \"\"\"Color palette that maps each class to RGB values.\n\n    This one is actually taken from ADE20k.\n    \"\"\"\n    # return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n    #         [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n    #         [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n    #         [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n    #         [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n    #         [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n    #         [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n    #         [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n    #         [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n    #         [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n    #         [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n    #         [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n    #         [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n    #         [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n    #         [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n    #         [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n    #         [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n    #         [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n    #         [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n    #         [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n    #         [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n    #         [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n    #         [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n    #         [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n    #         [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n    #         [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n    #         [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n    #         [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n    #         [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n    #         [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n    #         [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n    #         [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n    #         [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n    #         [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n    #         [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n    #         [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n    #         [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n    #         [102, 255, 0], [92, 0, 255]]\n\n    #since we only have 2 classes\n    return [[102, 255, 0], [92, 0, 255]]\n\npalette = color_palette()","metadata":{"id":"g7rq8UWhlS76","execution":{"iopub.status.busy":"2023-08-12T08:37:42.663512Z","iopub.execute_input":"2023-08-12T08:37:42.663875Z","iopub.status.idle":"2023-08-12T08:37:42.672155Z","shell.execute_reply.started":"2023-08-12T08:37:42.663845Z","shell.execute_reply":"2023-08-12T08:37:42.671207Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = np.array(image) * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"x88HQCNxM8t-","outputId":"455bd3f8-e651-4a9c-963b-a43e5e87ef6b","execution":{"iopub.status.busy":"2023-08-12T08:37:44.378314Z","iopub.execute_input":"2023-08-12T08:37:44.378761Z","iopub.status.idle":"2023-08-12T08:37:44.851088Z","shell.execute_reply.started":"2023-08-12T08:37:44.378713Z","shell.execute_reply":"2023-08-12T08:37:44.850144Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1000 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create PyTorch Dataset\n\nNext, we create a standard PyTorch dataset. Each item of the dataset consists of the image and corresponding ground truth segmentation map. We also include the original image + map (before preprocessing) in order to compute metrics like mIoU.","metadata":{"id":"d_XBkIDLl-hQ"}},{"cell_type":"code","source":"import numpy as np\nfrom torch.utils.data import Dataset\n\nclass ImageSegmentationDataset(Dataset):\n    \"\"\"Image segmentation dataset.\"\"\"\n\n    def __init__(self, dataset, transform):\n        \"\"\"\n        Args:\n            dataset\n        \"\"\"\n        self.dataset = dataset\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        original_image = np.array(self.dataset[idx]['image'])\n        original_segmentation_map = np.array(self.dataset[idx]['label'])\n        \n        # adding one bottom most pixel as 255 since processor/feature_extractor \n        # wont take labels without a positive (i.e. class: 1 or cloud)\n        uniques = np.unique(original_segmentation_map)\n        if sum(uniques) == 0:\n            original_segmentation_map[-1, -1] = 255\n        \n        transformed = self.transform(image=original_image, mask=original_segmentation_map)\n        image, segmentation_map = transformed['image'], transformed['mask']\n\n        # convert to C, H, W\n        image = image.transpose(2,0,1)\n\n        return image, segmentation_map, original_image, original_segmentation_map","metadata":{"execution":{"iopub.status.busy":"2023-08-12T08:37:46.151710Z","iopub.execute_input":"2023-08-12T08:37:46.152071Z","iopub.status.idle":"2023-08-12T08:37:49.070940Z","shell.execute_reply.started":"2023-08-12T08:37:46.152040Z","shell.execute_reply":"2023-08-12T08:37:49.069938Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The dataset accepts image transformations which can be applied on both the image and the map. Here we use Albumentations, to resize, randomly crop + flip and normalize them. Data augmentation is a widely used technique in computer vision to make the model more robust.","metadata":{"id":"ErruKkq5bftg"}},{"cell_type":"code","source":"# !pip install albumentations opencv-python","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\nADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\nADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n\ntrain_transform = A.Compose([\n#     A.LongestMaxSize(max_size=384),\n#     A.RandomCrop(width=100, height=100),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n])\n\ntest_transform = A.Compose([\n#     A.Resize(width=100, height=100),\n    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n])\n# train_dataset = ImageSegmentationDataset(train_ds)\ntrain_dataset = ImageSegmentationDataset(train_ds, transform=train_transform)\ntest_dataset = ImageSegmentationDataset(test_ds, transform=test_transform)\n# test_dataset = ImageSegmentationDataset(test_ds)","metadata":{"id":"aP3tzVTb8hgJ","execution":{"iopub.status.busy":"2023-08-12T08:37:49.075287Z","iopub.execute_input":"2023-08-12T08:37:49.076735Z","iopub.status.idle":"2023-08-12T08:37:50.739091Z","shell.execute_reply.started":"2023-08-12T08:37:49.076698Z","shell.execute_reply":"2023-08-12T08:37:50.737988Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# cnt = 0\n\n# for item in train_dataset:\n#     label = item[3]\n#     uniques = np.unique(label)\n#     if sum(uniques) == 0:\n#         cnt += 1\n\n# print(cnt)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:27:58.682302Z","iopub.execute_input":"2023-08-10T04:27:58.682769Z","iopub.status.idle":"2023-08-10T04:29:45.002842Z","shell.execute_reply.started":"2023-08-10T04:27:58.682730Z","shell.execute_reply":"2023-08-10T04:29:45.001531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my_array = np.array([\n#     [1,2,3],\n#     [4,5,6],\n#     [7,8,9]\n# ])\n\n# print(my_array.shape)\n\n# my_array[-1,-1] = -1\n\n# print(my_array)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:22:55.591690Z","iopub.execute_input":"2023-08-10T04:22:55.592857Z","iopub.status.idle":"2023-08-10T04:22:55.600120Z","shell.execute_reply.started":"2023-08-10T04:22:55.592818Z","shell.execute_reply":"2023-08-10T04:22:55.598921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for item in train_dataset:\n#   print(item)","metadata":{"id":"rE6a6NmpOAml","outputId":"5b3ae981-4b61-4856-c796-40fa44009c06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image, segmentation_map, _, _ = train_dataset[0]\nimage, segmentation_map, _, _ = train_dataset[222]\nprint(image.shape)\nprint(segmentation_map.shape)","metadata":{"id":"Dr99XeHUNg0m","outputId":"f90ea69a-2ea6-403e-f9fc-1d397fcd5804","execution":{"iopub.status.busy":"2023-08-12T08:37:53.811471Z","iopub.execute_input":"2023-08-12T08:37:53.811858Z","iopub.status.idle":"2023-08-12T08:37:53.846021Z","shell.execute_reply.started":"2023-08-12T08:37:53.811826Z","shell.execute_reply":"2023-08-12T08:37:53.844927Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(3, 384, 384)\n(384, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"# image, segmentation_map, _, _ = train_dataset[0]\n# print(image.shape)\n# print(segmentation_map.shape)","metadata":{"id":"JBE0WOQV8uXs","outputId":"7ab70a2a-0859-4d2a-d5cc-e42c1f2d9405"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A great way to check that our data augmentations are working well is by denormalizing the pixel values. So here we perform the inverse operation of Albumentations' normalize method and visualize the image:","metadata":{"id":"6tXh3ml3cFtO"}},{"cell_type":"code","source":"from PIL import Image\n\nunnormalized_image = (image * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nImage.fromarray(unnormalized_image)","metadata":{"id":"IYzfYUemNspT","outputId":"f124b253-333d-47fa-a283-acab366141ed","execution":{"iopub.status.busy":"2023-08-12T08:37:55.392067Z","iopub.execute_input":"2023-08-12T08:37:55.392499Z","iopub.status.idle":"2023-08-12T08:37:55.420949Z","shell.execute_reply.started":"2023-08-12T08:37:55.392463Z","shell.execute_reply":"2023-08-12T08:37:55.419162Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=384x384>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAGACAIAAAArpSLoAAABxElEQVR4nO3BgQAAAADDoPlTX+AIVQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHwDwdoAAfZN6/0AAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":"# from PIL import Image\n\n# unnormalized_image = (image * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n# unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n# unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n# Image.fromarray(unnormalized_image)","metadata":{"id":"PXmOsNRS8xle","outputId":"ddd47be1-716f-4a1a-e2d6-2f61adc8ab7f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This looks ok. Let's also verify whether the corresponding ground truth map is still ok.","metadata":{"id":"agSfvkzHcQVL"}},{"cell_type":"code","source":"segmentation_map.shape","metadata":{"id":"qnuSFGPRNysg","outputId":"ee178d2b-727d-49d6-db0b-032f78734b52","execution":{"iopub.status.busy":"2023-08-10T07:07:11.564939Z","iopub.execute_input":"2023-08-10T07:07:11.565645Z","iopub.status.idle":"2023-08-10T07:07:11.572758Z","shell.execute_reply.started":"2023-08-10T07:07:11.565612Z","shell.execute_reply":"2023-08-10T07:07:11.571190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# segmentation_map.shape","metadata":{"id":"VmnsFA35HlVj","outputId":"24869589-79e3-4311-ff2f-c309f20f0d08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [id2label[label] for label in np.unique(segmentation_map/255.0)]\nprint(labels)","metadata":{"id":"ZfLM_S40N0bK","outputId":"831afeae-75a6-4741-e9d9-7e531b0426f0","execution":{"iopub.status.busy":"2023-08-12T08:37:58.473878Z","iopub.execute_input":"2023-08-12T08:37:58.474489Z","iopub.status.idle":"2023-08-12T08:37:58.490144Z","shell.execute_reply.started":"2023-08-12T08:37:58.474445Z","shell.execute_reply":"2023-08-12T08:37:58.488540Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['non-cloud', 'cloud']\n","output_type":"stream"}]},{"cell_type":"code","source":"# labels = [id2label[label] for label in np.unique(segmentation_map)]\n# print(labels)","metadata":{"id":"95-nfotMICCq","outputId":"af45f503-1e3e-499d-9b38-20c2d1eadf82"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = np.moveaxis(image, 0, -1) * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"1MTYKEyfN392","outputId":"36404caa-76ce-4dc4-d5f3-0a1755e657ea","execution":{"iopub.status.busy":"2023-08-12T08:38:00.325015Z","iopub.execute_input":"2023-08-12T08:38:00.325722Z","iopub.status.idle":"2023-08-12T08:38:00.697908Z","shell.execute_reply.started":"2023-08-12T08:38:00.325688Z","shell.execute_reply":"2023-08-12T08:38:00.696808Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x1000 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n\n# color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n# for label, color in enumerate(palette):\n#     color_segmentation_map[segmentation_map == label, :] = color\n# # Convert to BGR\n# ground_truth_color_seg = color_segmentation_map[..., ::-1]\n\n# img = np.moveaxis(image, 0, -1) * 0.5 + ground_truth_color_seg * 0.5\n# img = img.astype(np.uint8)\n\n# plt.figure(figsize=(15, 10))\n# plt.imshow(img)\n# plt.show()","metadata":{"id":"cFv_KjVN87XK","outputId":"490540fa-8a40-463e-aca6-2236d97ca3c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok great!","metadata":{"id":"0HouWBIzcYwR"}},{"cell_type":"markdown","source":"## Create PyTorch DataLoaders\n\nNext we create PyTorch DataLoaders, which allow us to get batches of the dataset. For that we define a custom so-called \"collate function\", which PyTorch allows you to do. It's in this function that we'll use the preprocessor of MaskFormer, to turn the images + maps into the format that MaskFormer expects.\n\nIt's here that we make the paradigm shift that the MaskFormer authors introduced: the \"per-pixel\" annotations of the segmentation map will be turned into a set of binary masks and corresponding labels. It's this format on which we can train MaskFormer. MaskFormer namely casts any image segmentation task to this format.","metadata":{"id":"9wpLhlMWcaHl"}},{"cell_type":"code","source":"from transformers import MaskFormerImageProcessor,Mask2FormerImageProcessor, AutoImageProcessor\n\n# Create a preprocessor\n# preprocessor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\",\n#                                                   do_reduce_labels=False,\n#                                                   do_resize=False, do_rescale=False, do_normalize=False)\n\nsize = {'longest_edge':384, 'shortest_edge':384}\n\n#original\npreprocessor = Mask2FormerImageProcessor(ignore_index=0, \n                                        do_reduce_labels=False, \n                                        do_resize=False, \n                                        do_rescale=False, \n                                        do_normalize=True,\n                                        size=size)\n\n#my experiment\n# preprocessor = Mask2FormerImageProcessor(ignore_index=0, do_reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)","metadata":{"id":"N8-ThOC8ctsX","execution":{"iopub.status.busy":"2023-08-12T08:38:04.492338Z","iopub.execute_input":"2023-08-12T08:38:04.493043Z","iopub.status.idle":"2023-08-12T08:38:12.630295Z","shell.execute_reply.started":"2023-08-12T08:38:04.493008Z","shell.execute_reply":"2023-08-12T08:38:12.629252Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daba4c3e1bac42808715d46a45155545"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"preprocessor","metadata":{"execution":{"iopub.status.busy":"2023-08-12T08:38:12.632429Z","iopub.execute_input":"2023-08-12T08:38:12.633233Z","iopub.status.idle":"2023-08-12T08:38:12.639759Z","shell.execute_reply.started":"2023-08-12T08:38:12.633170Z","shell.execute_reply":"2023-08-12T08:38:12.638839Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Mask2FormerImageProcessor {\n  \"_max_size\": 1333,\n  \"do_normalize\": true,\n  \"do_reduce_labels\": false,\n  \"do_rescale\": false,\n  \"do_resize\": false,\n  \"ignore_index\": 0,\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_processor_type\": \"Mask2FormerImageProcessor\",\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"reduce_labels\": false,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 384,\n    \"shortest_edge\": 384\n  },\n  \"size_divisor\": 32\n}"},"metadata":{}}]},{"cell_type":"code","source":"# train_dataset[0]","metadata":{"id":"FYLs9ZC1wJ58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    inputs = list(zip(*batch))\n    images = inputs[0]\n    segmentation_maps = inputs[1]\n    # this function pads the inputs to the same size,\n    # and creates a pixel mask\n    # actually padding isn't required here since we are cropping\n    batch = preprocessor(\n        images,\n        segmentation_maps=segmentation_maps,\n        return_tensors=\"pt\",\n    )\n\n    batch[\"original_images\"] = inputs[2]\n    batch[\"original_segmentation_maps\"] = inputs[3]\n    \n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-08-12T08:38:12.641224Z","iopub.execute_input":"2023-08-12T08:38:12.641802Z","iopub.status.idle":"2023-08-12T08:38:12.651907Z","shell.execute_reply.started":"2023-08-12T08:38:12.641767Z","shell.execute_reply":"2023-08-12T08:38:12.650923Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n# batch size more than 4 causes CUDA out of memory error\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-08-12T08:38:12.670019Z","iopub.execute_input":"2023-08-12T08:38:12.670562Z","iopub.status.idle":"2023-08-12T08:38:12.678489Z","shell.execute_reply.started":"2023-08-12T08:38:12.670490Z","shell.execute_reply":"2023-08-12T08:38:12.677522Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# for item in train_dataloader:\n#     print(item.keys())\n#     break","metadata":{"execution":{"iopub.status.busy":"2023-08-09T14:11:16.679236Z","iopub.status.idle":"2023-08-09T14:11:16.679933Z","shell.execute_reply.started":"2023-08-09T14:11:16.679693Z","shell.execute_reply":"2023-08-09T14:11:16.679715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verify data (!!)\n\nNext, it's ALWAYS very important to check whether the data you feed to the model actually makes sense. It's one of the main principles of [this amazing blog post](http://karpathy.github.io/2019/04/25/recipe/), if you wanna debug your neural networks.\n\nLet's check the first batch, and its content.","metadata":{"id":"FTFi7K4eda5W"}},{"cell_type":"code","source":"import torch\n\nbatch = next(iter(train_dataloader))\n\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k,v.shape)\n  else:\n    print(k,v[0].shape)","metadata":{"id":"X8VC2-2dOVbo","outputId":"1babd1ea-f994-401f-ac04-612015ec975d","execution":{"iopub.status.busy":"2023-08-12T08:38:14.207336Z","iopub.execute_input":"2023-08-12T08:38:14.207706Z","iopub.status.idle":"2023-08-12T08:38:14.400166Z","shell.execute_reply.started":"2023-08-12T08:38:14.207676Z","shell.execute_reply":"2023-08-12T08:38:14.398965Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"pixel_values torch.Size([4, 3, 384, 384])\npixel_mask torch.Size([4, 384, 384])\nmask_labels torch.Size([1, 384, 384])\nclass_labels torch.Size([1])\noriginal_images (384, 384, 3)\noriginal_segmentation_maps (384, 384)\n","output_type":"stream"}]},{"cell_type":"code","source":"pixel_values = batch[\"pixel_values\"][0].numpy()\npixel_values.shape","metadata":{"id":"xPdCw9egO3tm","outputId":"a9b9a4a7-54c6-4e8e-f07e-04c1c73c16ed","execution":{"iopub.status.busy":"2023-08-12T08:38:17.215653Z","iopub.execute_input":"2023-08-12T08:38:17.216019Z","iopub.status.idle":"2023-08-12T08:38:17.230562Z","shell.execute_reply.started":"2023-08-12T08:38:17.215989Z","shell.execute_reply":"2023-08-12T08:38:17.229380Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(3, 384, 384)"},"metadata":{}}]},{"cell_type":"markdown","source":"Again, let's denormalize an image and see what we got.","metadata":{"id":"IvYfrGG5dsCF"}},{"cell_type":"code","source":"unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\nunnormalized_image = (unnormalized_image * 255).astype(np.uint8)\nunnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\nImage.fromarray(unnormalized_image)","metadata":{"id":"RBYyIpUcO58-","execution":{"iopub.status.busy":"2023-08-11T04:18:53.904318Z","iopub.execute_input":"2023-08-11T04:18:53.904783Z","iopub.status.idle":"2023-08-11T04:18:53.938057Z","shell.execute_reply.started":"2023-08-11T04:18:53.904744Z","shell.execute_reply":"2023-08-11T04:18:53.937114Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=RGB size=384x384>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAGACAIAAAArpSLoAAAE0ElEQVR4nO3UIQEAIADAMCAT/dNQAkeMC7YEV5/n7gFQWHUA8C8DAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQMaAgIwBARkDAjIGBGQMCMgYEJAxICBjQEDGgICMAQEZAwIyBgRkDAjIGBCQMSAgY0BAxoCAjAEBGQMCMgYEZAwIyBgQkDEgIGNAQOYBQW4FETk7R6QAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":"# unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n# unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n# unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n# Image.fromarray(unnormalized_image)","metadata":{"id":"oYXbVJtkJSL8","outputId":"e4bb5f73-3257-4e96-b610-ef60d16ac65f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's verify the corresponding binary masks + class labels.","metadata":{"id":"6FAF5elmd3Xc"}},{"cell_type":"code","source":"# verify class labels\nlabels = [id2label[label] for label in (batch[\"class_labels\"][0]/255.0).tolist()]\nprint(labels)","metadata":{"id":"vdom-jV8O-KH","outputId":"4572828b-c166-4453-fe95-2fb4a5df3350","execution":{"iopub.status.busy":"2023-08-11T04:19:09.243529Z","iopub.execute_input":"2023-08-11T04:19:09.244223Z","iopub.status.idle":"2023-08-11T04:19:09.255375Z","shell.execute_reply.started":"2023-08-11T04:19:09.244184Z","shell.execute_reply":"2023-08-11T04:19:09.254095Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"['cloud']\n","output_type":"stream"}]},{"cell_type":"code","source":"# # verify class labels\n# labels = [id2label[label] for label in batch[\"class_labels\"][0].tolist()]\n# print(labels)","metadata":{"id":"xns61BlyJjTq","outputId":"34d8f469-fd99-456c-9a17-0b9e9a24fbe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify mask labels\nbatch[\"mask_labels\"][0].shape","metadata":{"id":"5HdJFbd_O_5S","outputId":"06a1c4ac-54ef-4ab5-de83-ef37a76cae25","execution":{"iopub.status.busy":"2023-08-11T04:19:07.323516Z","iopub.execute_input":"2023-08-11T04:19:07.324693Z","iopub.status.idle":"2023-08-11T04:19:07.331531Z","shell.execute_reply.started":"2023-08-11T04:19:07.324644Z","shell.execute_reply":"2023-08-11T04:19:07.330469Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 384, 384])"},"metadata":{}}]},{"cell_type":"code","source":"# # verify mask labels\n# batch[\"mask_labels\"][0].shape","metadata":{"id":"aOOvaUgpJsu-","outputId":"e302b889-0af0-485a-86d6-5a5cb759ac37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_mask(labels, label_name):\n  print(\"Label:\", label_name)\n  idx = labels.index(label_name)\n\n  visual_mask = (batch[\"mask_labels\"][0][idx].bool().numpy() * 255).astype(np.uint8)\n  return Image.fromarray(visual_mask)","metadata":{"id":"PeYuEFhDKkmI","execution":{"iopub.status.busy":"2023-08-11T04:19:01.082494Z","iopub.execute_input":"2023-08-11T04:19:01.082894Z","iopub.status.idle":"2023-08-11T04:19:01.088497Z","shell.execute_reply.started":"2023-08-11T04:19:01.082863Z","shell.execute_reply":"2023-08-11T04:19:01.087497Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"visualize_mask(labels, \"cloud\")","metadata":{"id":"myAAfyKZPFP1","outputId":"d526ae0c-a250-47d9-8d9a-e4dd54d92d75","execution":{"iopub.status.busy":"2023-08-11T04:19:13.275698Z","iopub.execute_input":"2023-08-11T04:19:13.276696Z","iopub.status.idle":"2023-08-11T04:19:13.287777Z","shell.execute_reply.started":"2023-08-11T04:19:13.276655Z","shell.execute_reply":"2023-08-11T04:19:13.286729Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Label: cloud\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<PIL.Image.Image image mode=L size=384x384>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAGACAAAAACBrOpjAAAApUlEQVR4nO3BAQEAAACCIP+friNAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABQ7UKdAQCff+yvAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":"# visualize_mask(labels, \"flat-road\")","metadata":{"id":"zZrDP-ozK8ze","outputId":"a8b2cd29-b962-40fd-ef8d-06725059915e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define model\n\nNext, we define the model. We equip the model with pretrained weights from the ðŸ¤— hub. We will replace only the classification head. For that we provide the id2label mapping, and specify to ignore mismatches keys to replace the already fine-tuned classification head.","metadata":{"id":"wvv5EaU5qi9M"}},{"cell_type":"code","source":"from transformers import MaskFormerForInstanceSegmentation, Mask2FormerForUniversalSegmentation\n\n# Replace the head of the pre-trained model\n\nmodel = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\",\n                                                            id2label=id2label,\n                                                            ignore_mismatched_sizes=True)\n\n\n# model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-ade\",\n#                                                           id2label=id2label,\n#                                                           ignore_mismatched_sizes=True)","metadata":{"id":"IMqBGmsFkfyB","outputId":"d9c9ad37-ad3f-4a04-fbc5-f2cbdace6110","execution":{"iopub.status.busy":"2023-08-12T08:43:38.951916Z","iopub.execute_input":"2023-08-12T08:43:38.952323Z","iopub.status.idle":"2023-08-12T08:43:42.281408Z","shell.execute_reply.started":"2023-08-12T08:43:38.952290Z","shell.execute_reply":"2023-08-12T08:43:42.280431Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/82.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d3ddf3b1854028a1b3cdb6c311e276"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"006c778d1c8746a1af6c185d2a411d42"}},"metadata":{}},{"name":"stderr","text":"Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-ade-semantic and are newly initialized because the shapes did not match:\n- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\n- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([3]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"See also the warning here: it's telling us that we are\n\n1.   List item\n2.   List item\n\nonly replacing the class_predictor, which makes sense. As it's the only parameters that we will train from scratch.","metadata":{"id":"cDf95us4eXeZ"}},{"cell_type":"markdown","source":"## Compute initial loss\n\nAnother good way to debug neural networks is to verify the initial loss, see if it makes sense.","metadata":{"id":"oQilHqCYLJx0"}},{"cell_type":"code","source":"# v = batch[\"class_labels\"]\n\n# v = [t / 255.0 for t in v]","metadata":{"id":"zpDJ8pD6gbRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# v","metadata":{"id":"AN8joIVLhGxe","outputId":"0df061fa-ab99-430a-9e58-e0016f5b80b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# v = batch[\"class_labels\"]\n\n# v = [t / 255.0 for t in v]\n\n# new_v = []\n\n# for t in v:\n#   new_t = torch.tensor(t, dtype=torch.int64)\n#   new_v.append(new_t)","metadata":{"id":"RT-20EWxiaWe","outputId":"83f98432-5bf8-492b-8033-08a60e6612e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [torch.tensor([1.], dtype=torch.uint8)]*2","metadata":{"id":"mD6OP4chYiFo","outputId":"f5b595a0-75cb-46a0-e12a-e1702a5a0c81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\nfrom PIL import Image\nimport requests\n\n# load MaskFormer fine-tuned on COCO panoptic segmentation\nfeature_extractor = MaskFormerFeatureExtractor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\", \n                                                               size={'longest_edge':384, 'shortest_edge':383}, \n                                                               ignore_index=0)","metadata":{"id":"zoFayrmCgico","outputId":"35757a0f-9a46-44ac-d430-5eb6e98cb229","execution":{"iopub.status.busy":"2023-08-12T08:43:45.707385Z","iopub.execute_input":"2023-08-12T08:43:45.707763Z","iopub.status.idle":"2023-08-12T08:43:45.937849Z","shell.execute_reply.started":"2023-08-12T08:43:45.707729Z","shell.execute_reply":"2023-08-12T08:43:45.936780Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce2567638194168bce10633298b0e0b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/maskformer/feature_extraction_maskformer.py:28: FutureWarning: The class MaskFormerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use MaskFormerImageProcessor instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/models/maskformer/image_processing_maskformer.py:419: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_extractor","metadata":{"execution":{"iopub.status.busy":"2023-08-12T08:45:48.473098Z","iopub.execute_input":"2023-08-12T08:45:48.474174Z","iopub.status.idle":"2023-08-12T08:45:48.480526Z","shell.execute_reply.started":"2023-08-12T08:45:48.474139Z","shell.execute_reply":"2023-08-12T08:45:48.479562Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"MaskFormerFeatureExtractor {\n  \"_max_size\": 2048,\n  \"do_normalize\": true,\n  \"do_reduce_labels\": false,\n  \"do_rescale\": true,\n  \"do_resize\": true,\n  \"ignore_index\": 0,\n  \"image_mean\": [\n    0.48500001430511475,\n    0.4560000002384186,\n    0.4059999883174896\n  ],\n  \"image_processor_type\": \"MaskFormerFeatureExtractor\",\n  \"image_std\": [\n    0.2290000021457672,\n    0.2239999920129776,\n    0.22499999403953552\n  ],\n  \"num_labels\": 150,\n  \"resample\": 2,\n  \"rescale_factor\": 0.00392156862745098,\n  \"size\": {\n    \"longest_edge\": 384,\n    \"shortest_edge\": 383\n  },\n  \"size_divisor\": 32\n}"},"metadata":{}}]},{"cell_type":"code","source":"device = \"cuda\"\n\nimages, labels = batch['original_images'], batch['original_segmentation_maps']\n\n# first convert to np array then to tensor... because list to tensor is a slow operation\nimages = np.array(images)\nimages = torch.tensor(images)\nlabels = np.array(labels)\nlabels = torch.tensor(labels)/255\n\nimages.to(device)\nlabels.to(device)\nmodel.to(device)\n\ninputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n# print(inputs)\n\ninputs['mask_labels'] = torch.stack(inputs['mask_labels'])\ninputs['class_labels'] = torch.stack(inputs['class_labels'])\ninputs['pixel_values'] = inputs['pixel_values'].float()\n# print(inputs)\ninputs.to(device)\n\noutputs = model(**inputs)\n\nprint(\"done!\")","metadata":{"id":"HCKuE7eXYLCv","outputId":"fe863f37-e142-4a89-bfea-66047e626e6d","execution":{"iopub.status.busy":"2023-08-12T08:45:53.049823Z","iopub.execute_input":"2023-08-12T08:45:53.050240Z","iopub.status.idle":"2023-08-12T08:46:03.744303Z","shell.execute_reply.started":"2023-08-12T08:45:53.050179Z","shell.execute_reply":"2023-08-12T08:46:03.743303Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"done!\n","output_type":"stream"}]},{"cell_type":"code","source":"# [(i//255) for i in batch[\"class_labels\"]]","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:31:30.597573Z","iopub.execute_input":"2023-08-09T13:31:30.598434Z","iopub.status.idle":"2023-08-09T13:31:30.602830Z","shell.execute_reply.started":"2023-08-09T13:31:30.598399Z","shell.execute_reply":"2023-08-09T13:31:30.601717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outputs = model(batch[\"pixel_values\"].float(),\n#                 class_labels=[(i//255) for i in batch[\"class_labels\"]],\n#                 mask_labels=batch[\"mask_labels\"])","metadata":{"id":"CK2IhtcOLLEA","execution":{"iopub.status.busy":"2023-08-09T13:31:26.498837Z","iopub.execute_input":"2023-08-09T13:31:26.499221Z","iopub.status.idle":"2023-08-09T13:31:26.504107Z","shell.execute_reply.started":"2023-08-09T13:31:26.499188Z","shell.execute_reply":"2023-08-09T13:31:26.503127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.loss","metadata":{"id":"357KLDNhjUj1","outputId":"2b555db0-4640-4a62-d54b-5d70b9225176","execution":{"iopub.status.busy":"2023-08-12T08:48:48.411456Z","iopub.execute_input":"2023-08-12T08:48:48.411846Z","iopub.status.idle":"2023-08-12T08:48:48.425301Z","shell.execute_reply.started":"2023-08-12T08:48:48.411815Z","shell.execute_reply":"2023-08-12T08:48:48.424321Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"tensor([36.3112], device='cuda:0', grad_fn=<AddBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"# outputs.loss","metadata":{"id":"sMsSWMO-LQZB","outputId":"dd8b10a4-e0ff-409f-e410-600454becb2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n\nIt's time to train the model! We'll use the mIoU metric to track progress.","metadata":{"id":"oP4Aj19v1Da_"}},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"id":"xNIijv2LQCU4","outputId":"ae0df21a-8304-41e9-f09e-e6aead9497f7","execution":{"iopub.status.busy":"2023-08-12T08:48:52.478059Z","iopub.execute_input":"2023-08-12T08:48:52.478766Z","iopub.status.idle":"2023-08-12T08:49:05.324682Z","shell.execute_reply.started":"2023-08-12T08:48:52.478731Z","shell.execute_reply":"2023-08-12T08:49:05.323270Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\nmean_iou = evaluate.load(\"mean_iou\")\nprecision = evaluate.load(\"precision\")\nrecall = evaluate.load(\"recall\")\nf1 = evaluate.load(\"f1\")\naccuracy = evaluate.load(\"accuracy\")\n# clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\", \"mean_iou\", \"recall\"])","metadata":{"id":"nuoNVUSKP8u1","execution":{"iopub.status.busy":"2023-08-12T08:49:05.328074Z","iopub.execute_input":"2023-08-12T08:49:05.328509Z","iopub.status.idle":"2023-08-12T08:49:10.830775Z","shell.execute_reply.started":"2023-08-12T08:49:05.328469Z","shell.execute_reply":"2023-08-12T08:49:10.829730Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea84e1599bfc4c54aa2d62d5c1b435d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e62cfe8cfde4e84b9ab9cdd3ca65ea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04a977a1a55f4ad1b58a517750674c9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43282e0b6044f51954ac5e7a1cdf072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed230ee10b9a4255b1d431e162c848f9"}},"metadata":{}}]},{"cell_type":"code","source":"# batch[\"pixel_values\"].size(0)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T04:56:22.573967Z","iopub.execute_input":"2023-08-10T04:56:22.574677Z","iopub.status.idle":"2023-08-10T04:56:22.581097Z","shell.execute_reply.started":"2023-08-10T04:56:22.574644Z","shell.execute_reply":"2023-08-10T04:56:22.580081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = \"cuda\"\nmodel.to(device)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=5e-5)\n# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n\nrunning_loss = 0.0\nnum_samples = 0\nfor epoch in range(30):\n  print(\"Epoch:\", epoch)\n  model.train()\n  for idx, batch in enumerate(tqdm(train_dataloader)):\n      # Reset the parameter gradients\n      optimizer.zero_grad()\n\n      images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n      images = np.array(images)\n      images = torch.tensor(images)\n      labels = np.array(labels)\n      labels = torch.tensor(labels)/255\n\n      images.to(device)\n      labels.to(device)\n      model.to(device)\n\n      inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#       print(inputs)\n\n      inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n      inputs['class_labels'] = torch.stack(inputs['class_labels'])\n      inputs['pixel_values'] = inputs['pixel_values'].float()\n      inputs.to(device)\n\n      outputs = model(**inputs)\n\n      # Backward propagation\n      loss = outputs.loss\n      loss.backward()\n\n      batch_size = batch[\"pixel_values\"].size(0)\n      running_loss += loss.item()\n      num_samples += batch_size\n\n      if idx % 50 == 0:\n        print(\"Loss:\", running_loss/num_samples)\n\n      # Optimization\n      optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-08-12T08:51:50.948422Z","iopub.execute_input":"2023-08-12T08:51:50.948814Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1680 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9b6ada148d4cf8b04935db5e03c200"}},"metadata":{}},{"name":"stdout","text":"Loss: 16.24918556213379\nLoss: 7.591471573885749\nLoss: 7.032444453475499\nLoss: 6.964656081420697\nLoss: 6.583544077564828\nLoss: 6.3651260094813615\nLoss: 6.2638564157327545\nLoss: 6.236922338817195\nLoss: 6.198635761577292\nLoss: 6.070553713785835\nLoss: 6.026470329471215\nLoss: 6.0053587282201555\nLoss: 5.971224181465619\nLoss: 5.888044872591572\nLoss: 5.8282277677606755\nLoss: 5.7827561544514845\nLoss: 5.741161393315605\nLoss: 5.678629008639433\nLoss: 5.637183274069055\nLoss: 5.578293408254469\nLoss: 5.54637959167793\nLoss: 5.521533828480373\nLoss: 5.484251960963146\nLoss: 5.457453549995721\nLoss: 5.452564703534783\nLoss: 5.4177478993062875\nLoss: 5.404040092738384\nLoss: 5.391753125817224\nLoss: 5.367371164373973\nLoss: 5.330707745951345\nLoss: 5.315707053644192\nLoss: 5.295661170086039\nLoss: 5.259522295906572\nLoss: 5.245327128922123\nEpoch: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1680 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a262a1ca7ade4d519cc3252824be7176"}},"metadata":{}},{"name":"stdout","text":"Loss: 5.222014302586176\nLoss: 5.208030709154271\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, 'mask2former-model-v2-loss-3.63.pth')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T10:24:35.398814Z","iopub.execute_input":"2023-08-11T10:24:35.399993Z","iopub.status.idle":"2023-08-11T10:24:35.905728Z","shell.execute_reply.started":"2023-08-11T10:24:35.399953Z","shell.execute_reply":"2023-08-11T10:24:35.904527Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model.eval()\nfor idx, batch in enumerate(tqdm(test_dataloader)):\n    if idx > 5:\n        break\n    images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n    images = np.array(images)\n    images = torch.tensor(images)\n    labels = np.array(labels)\n    labels = torch.tensor(labels)/255\n\n    images.to(device)\n    labels.to(device)\n    model.to(device)\n\n    inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#     print(inputs)\n    inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n    inputs['class_labels'] = torch.stack(inputs['class_labels'])  \n    inputs['pixel_values'] = inputs['pixel_values'].float()\n    inputs.to(device)\n\n    # Forward pass\n    with torch.no_grad():\n      # outputs = model(pixel_values=pixel_values.to(device))\n      outputs = model(**inputs)\n\n    # get original images\n    # original_images = batch[\"original_images\"]\n    target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n    # predict segmentation maps\n    predicted_segmentation_maps = feature_extractor.post_process_semantic_segmentation(outputs,\n                                                                                  target_sizes=target_sizes)\n    \n#     print(outputs.keys())\n    # get ground truth segmentation maps\n    # ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n    for preds in predicted_segmentation_maps:\n        preds.int()\n        preds.cpu()\n    for label in labels:\n        label.int()\n        label.cpu()\n        \n#     print(predicted_segmentation_maps)\n#     print(labels)\n    \n    # removing all values into a list of ints because the evaluate library expects it that way.\n    labels_for_evaluation = []\n    \n    for label in labels:\n        labels_for_evaluation.append(label.view(-1))\n    \n    labels_for_evaluation = torch.cat(labels_for_evaluation, dim = 0)\n    labels_for_evaluation.int()\n    \n    pred_labels_for_evaluation = []\n    \n    for preds in predicted_segmentation_maps:\n        pred_labels_for_evaluation.append(label.view(-1))\n    \n    pred_labels_for_evaluation = torch.cat(pred_labels_for_evaluation, dim = 0)\n    pred_labels_for_evaluation.int()\n    \n    #for mean iou calculation... pred maps and labels must be same shape\n    labels_list = []\n\n    for i in range(labels.size(0)):\n        labels[i].int()\n        labels_list.append(labels[i])\n        \n    \n    mean_iou.add_batch(references=labels_list, predictions=predicted_segmentation_maps)\n    precision.add_batch(references=labels_for_evaluation, predictions=pred_labels_for_evaluation)\n    recall.add_batch(references=labels_for_evaluation, predictions=pred_labels_for_evaluation)\n    f1.add_batch(references=labels_for_evaluation, predictions=pred_labels_for_evaluation)\n    accuracy.add_batch(references=labels_for_evaluation, predictions=pred_labels_for_evaluation)\n#     clf_metrics.add_batch(references=labels, predictions=predicted_segmentation_maps)\n\n  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n  # so if you're interested, feel free to print them as well\nprint(\"mean IoU: \", mean_iou.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])\nprint(\"precision\", precision.compute()['precision'])\nprint(\"accuracy\", accuracy.compute()['accuracy'])\nprint(\"f1\", f1.compute()['f1'])\nprint(\"recall\", recall.compute()['recall'])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T11:10:55.112557Z","iopub.execute_input":"2023-08-11T11:10:55.112926Z","iopub.status.idle":"2023-08-11T11:13:16.384111Z","shell.execute_reply.started":"2023-08-11T11:10:55.112892Z","shell.execute_reply":"2023-08-11T11:13:16.383088Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/420 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00bcfc631e749c78d7f1ce7ef8eb27b"}},"metadata":{}},{"name":"stderr","text":"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:259: RuntimeWarning: invalid value encountered in divide\n  iou = total_area_intersect / total_area_union\n/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--mean_iou/08bc20f4f895f3caf75fb9e3fada1404bded3c3265243d05327cbb3b9326ffe9/mean_iou.py:260: RuntimeWarning: invalid value encountered in divide\n  acc = total_area_intersect / total_area_label\n","output_type":"stream"},{"name":"stdout","text":"mean IoU:  1.0\nprecision 0.25989799519384277\naccuracy 0.7442448933919271\nf1 0.29082780359497634\nrecall 0.3301138213117466\n","output_type":"stream"}]},{"cell_type":"raw","source":"import torch\nfrom tqdm.auto import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = \"cuda\"\nmodel.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\nrunning_loss = 0.0\nnum_samples = 0\nfor epoch in range(2):\n  print(\"Epoch:\", epoch)\n  model.train()\n  for idx, batch in enumerate(tqdm(train_dataloader)):\n      # Reset the parameter gradients\n      optimizer.zero_grad()\n\n      images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n      images = np.array(images)\n      images = torch.tensor(images)\n      labels = np.array(labels)\n      labels = torch.tensor(labels)/255\n\n      images.to(device)\n      labels.to(device)\n      model.to(device)\n\n      inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#       print(inputs)\n\n      inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n      inputs['class_labels'] = torch.stack(inputs['class_labels'])\n      inputs['pixel_values'] = inputs['pixel_values'].float()\n      inputs.to(device)\n\n      outputs = model(**inputs)\n\n      # Backward propagation\n      loss = outputs.loss\n      loss.backward()\n\n      batch_size = batch[\"pixel_values\"].size(0)\n      running_loss += loss.item()\n      num_samples += batch_size\n\n      if idx % 100 == 0:\n        print(\"Loss:\", running_loss/num_samples)\n\n      # Optimization\n      optimizer.step()\n      break\n\n  model.eval()\n  for idx, batch in enumerate(tqdm(test_dataloader)):\n    if idx > 1:\n      break\n    images, labels = batch['original_images'], batch['original_segmentation_maps']\n\n    images = np.array(images)\n    images = torch.tensor(images)\n    labels = np.array(labels)\n    labels = torch.tensor(labels)/255\n\n    images.to(device)\n    labels.to(device)\n    model.to(device)\n\n    inputs = feature_extractor(images = images, segmentation_maps = labels, return_tensors = 'pt')\n#     print(inputs)\n    inputs['mask_labels'] = torch.stack(inputs['mask_labels'])\n    inputs['class_labels'] = torch.stack(inputs['class_labels'])  \n    inputs['pixel_values'] = inputs['pixel_values'].float()\n    inputs.to(device)\n\n    # Forward pass\n    with torch.no_grad():\n      # outputs = model(pixel_values=pixel_values.to(device))\n      outputs = model(**inputs)\n\n    # get original images\n    # original_images = batch[\"original_images\"]\n    target_sizes = [(image.shape[0], image.shape[1]) for image in images]\n    # predict segmentation maps\n    predicted_segmentation_maps = feature_extractor.post_process_semantic_segmentation(outputs,\n                                                                                  target_sizes=target_sizes)\n    \n    print(outputs.keys())\n    # get ground truth segmentation maps\n    # ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n    for preds in predicted_segmentation_maps:\n        preds.int()\n        preds.cpu()\n    for label in labels:\n        label.int()\n        label.cpu()\n        \n#     print(predicted_segmentation_maps)\n#     print(labels)\n    \n    # removing all values into a list of ints because the evaluate library expects it that way.\n    labels_for_evaluation = []\n    \n    for label in labels:\n        labels_for_evaluation.append(label.view(-1))\n    \n    labels_for_evaluation = torch.cat(labels_for_evaluation, dim = 0)\n    labels_for_evaluation.int()\n    \n    pred_labels_for_evaluation = []\n    \n    for preds in predicted_segmentation_maps:\n        pred_labels_for_evaluation.append(label.view(-1))\n    \n    pred_labels_for_evaluation = torch.cat(pred_labels_for_evaluation, dim = 0)\n    pred_labels_for_evaluation.int()\n    \n    print(labels)\n    print(predicted_segmentation_maps)\n    \n    mean_iou.add_batch(references=labels, predictions=predicted_segmentation_maps)\n    precision.add_batch(references=labels_for_evaluation, predictions=pred_labels_for_evaluation)\n#     clf_metrics.add_batch(references=labels, predictions=predicted_segmentation_maps)\n\n  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n  # so if you're interested, feel free to print them as well\n  print(\"mean IoU: \", mean_iou.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])\n  print(\"precision: \", precision.compute()['precision'])\n#   print(clf_metrics.compute(num_labels = len(id2label), ignore_index = 0))","metadata":{"id":"WsTo9eRRj96f","outputId":"130e5400-659b-4d75-f7b0-23450acadc8f","execution":{"iopub.status.busy":"2023-08-10T10:15:23.575867Z","iopub.execute_input":"2023-08-10T10:15:23.576246Z","iopub.status.idle":"2023-08-10T10:17:03.753201Z","shell.execute_reply.started":"2023-08-10T10:15:23.576215Z","shell.execute_reply":"2023-08-10T10:17:03.752132Z"}}},{"cell_type":"code","source":"print(labels_for_evaluation)\nl = torch.cat(labels_for_evaluation, dim=0)\nl.int()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T07:52:11.005892Z","iopub.execute_input":"2023-08-10T07:52:11.006509Z","iopub.status.idle":"2023-08-10T07:52:11.017925Z","shell.execute_reply.started":"2023-08-10T07:52:11.006475Z","shell.execute_reply":"2023-08-10T07:52:11.016763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from tqdm.auto import tqdm\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\n# running_loss = 0.0\n# num_samples = 0\n# for epoch in range(3):\n#   print(\"Epoch:\", epoch)\n#   model.train()\n#   for idx, batch in enumerate(tqdm(train_dataloader)):\n#       # Reset the parameter gradients\n#       optimizer.zero_grad()\n\n#       # Forward pass\n#       outputs = model(\n#           pixel_values=batch[\"pixel_values\"].to(device),\n#           mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n#           class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n#       )\n\n#       # Backward propagation\n#       loss = outputs.loss\n#       loss.backward()\n\n#       batch_size = batch[\"pixel_values\"].size(0)\n#       running_loss += loss.item()\n#       num_samples += batch_size\n\n#       if idx % 100 == 0:\n#         print(\"Loss:\", running_loss/num_samples)\n\n#       # Optimization\n#       optimizer.step()\n\n#   model.eval()\n#   for idx, batch in enumerate(tqdm(test_dataloader)):\n#     if idx > 5:\n#       break\n\n#     pixel_values = batch[\"pixel_values\"]\n\n#     # Forward pass\n#     with torch.no_grad():\n#       outputs = model(pixel_values=pixel_values.to(device))\n\n#     # get original images\n#     original_images = batch[\"original_images\"]\n#     target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n#     # predict segmentation maps\n#     predicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs,\n#                                                                                   target_sizes=target_sizes)\n\n#     # get ground truth segmentation maps\n#     ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n\n#     metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n\n#   # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n#   # so if you're interested, feel free to print them as well\n#   print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])","metadata":{"id":"IBcCOO_91EKI","outputId":"b024f86a-e010-490c-a84f-b5a2586b67be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hi\")","metadata":{"execution":{"iopub.status.busy":"2023-08-09T16:03:10.111190Z","iopub.execute_input":"2023-08-09T16:03:10.111603Z","iopub.status.idle":"2023-08-09T16:03:10.118158Z","shell.execute_reply.started":"2023-08-09T16:03:10.111571Z","shell.execute_reply":"2023-08-09T16:03:10.116232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nAfter training, we can use the model to make predictions on new data.\n\nLet's showcase this one of the examples of a test batch.","metadata":{"id":"aWO7PYmL-B5N"}},{"cell_type":"code","source":"# let's take the first test batch\nbatch = next(iter(test_dataloader))\nfor k,v in batch.items():\n  if isinstance(v, torch.Tensor):\n    print(k,v.shape)\n  else:\n    print(k,len(v))","metadata":{"id":"MNxV_19j-Crl","outputId":"758a30b0-bcc5-44f9-8dc4-b5afe3f6112d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# forward pass\nwith torch.no_grad():\n  outputs = model(batch[\"pixel_values\"].to(device))","metadata":{"id":"PqfrPxfe-S8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_images = batch[\"original_images\"]\ntarget_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n# predict segmentation maps\npredicted_segmentation_maps = preprocessor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)","metadata":{"id":"j5fC9KrU-aNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = batch[\"original_images\"][0]\nImage.fromarray(image)","metadata":{"id":"adqqcnpl-jy2","outputId":"a584fc50-346d-411b-f7a1-7ee6e3c6a278"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nsegmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = image * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"tXZrYMNW-yrN","outputId":"d0c2847a-8ac7-40ac-f47a-37a41280a962"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare to the ground truth:","metadata":{"id":"hf0WXFT5_Np8"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nsegmentation_map = batch[\"original_segmentation_maps\"][0]\n\ncolor_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\nfor label, color in enumerate(palette):\n    color_segmentation_map[segmentation_map == label, :] = color\n# Convert to BGR\nground_truth_color_seg = color_segmentation_map[..., ::-1]\n\nimg = image * 0.5 + ground_truth_color_seg * 0.5\nimg = img.astype(np.uint8)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(img)\nplt.show()","metadata":{"id":"DKZ_3oF__Pll","outputId":"e37f8a4c-34cb-4c65-c198-d73eb83aa8c6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didn't do a lot of training (only 2 epochs), and results don't look too bad. I'd suggest checking the paper to find all details regarding training hyperparameters (number of epochs, learning rate, etc.).","metadata":{"id":"d9u_iFnmjPVo"}},{"cell_type":"code","source":"","metadata":{"id":"M0MIQub-RjUH"},"execution_count":null,"outputs":[]}]}